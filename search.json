[{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"An Introduction to `PGRdup` Package","text":"PGRdup R package facilitate search probable/possible duplicate accessions Plant Genetic Resources (PGR) collections using passport databases. Primarily package implements workflow (Fig. 1) designed fetch groups sets germplasm accessions similar passport data particularly fields associated accession names within across PGR passport databases. offers suite functions data pre-processing, creation searchable Key Word Context (KWIC) index keywords associated accession records identification probable duplicate sets fuzzy, phonetic semantic matching keywords. also functions enable user review, modify validate probable duplicate sets retrieved. goal document introduce users functions familiarise workflow intended fetch probable duplicate sets. document assumes basic knowledge R programming language. functions package primarily built using R packages data.table, igraph, stringdist stringi.   Fig. 1. PGRdup workflow associated functions","code":""},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"version-history","dir":"Articles","previous_headings":"","what":"Version History","title":"An Introduction to `PGRdup` Package","text":"current version package 0.2.4.0.9000. previous versions follows. Table 1. Version history PGRdup R package. know detailed history changes use news(package='PGRdup').","code":""},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"An Introduction to `PGRdup` Package","text":"package can installed using following functions: Uninstalled dependencies (packages PGRdup depends viz- data.table, igraph, stringdist stringi also installed argument dependencies=TRUE. package can loaded using function","code":"# Install from CRAN install.packages('PGRdup', dependencies=TRUE) library(PGRdup)"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"data-format","dir":"Articles","previous_headings":"","what":"Data Format","title":"An Introduction to `PGRdup` Package","text":"package essentially designed operate PGR passport data present data frame object, row holding one record columns representing attribute fields. example, consider dataset GN1000 supplied along package. passport data exists excel sheet, can first converted comma-separated values (csv) file tab delimited file easily imported R environment using base functions read.csv read.table respectively. Similarly read_csv() read_tsv() readr package can also used. Alternatively, package readxl can used directly read data excel. case large csv files, function fread data.table package can used rapidly load data. PGR passport data database management system (DBMS), required table can imported data frame R. using appropriate R-database interface package. example dbConnect MySQL, ROracle Oracle etc. PGR data downloaded genesys database Darwin Core - Germplasm zip archive can imported R environment flat file data.frame using read.genesys function.","code":"library(PGRdup) -------------------------------------------------------------------------------- Welcome to PGRdup version 0.2.4.0.9000   # To know how to use this package type:   browseVignettes(package = 'PGRdup')   for the package vignette.  # To know whats new in this version type:   news(package='PGRdup')   for the NEWS file.  # To cite the methods in the package type:   citation(package='PGRdup')  # To suppress this message use:   suppressPackageStartupMessages(library(PGRdup)) -------------------------------------------------------------------------------- # Load the dataset to the environment data(GN1000) # Show the class of the object class(GN1000) [1] \"data.frame\" # View the first few records in the data frame head(GN1000) CommonName    BotanicalName NationalID                CollNo   DonorID 1  Groundnut Arachis hypogaea   EC100277 Shulamith/ NRCG-14555  ICG-4709 2  Groundnut Arachis hypogaea   EC100280                    NC   ICG5288 3  Groundnut Arachis hypogaea   EC100281               MALIMBA   ICG5289 4  Groundnut Arachis hypogaea   EC100713            EC 100713;   ICG5296 5  Groundnut Arachis hypogaea   EC100715             EC 100715   ICG5298 6  Groundnut Arachis hypogaea   EC100716                        ICG-3150   OtherID1  OtherID2 BioStatus            SourceCountry TransferYear 1           U4-47-12  Landrace                   Israel         2014 2      NCS      NC 5  Landrace United States of America         2004 3          EC 100281  Landrace                   Malawi         2004 4              STARR  Landrace United States of America         2004 5              COMET  Landrace United States of America         2004 6          ARGENTINE  Landrace United States of America         2014 # Import the DwC-Germplasm zip archive \"genesys-accessions-filtered.zip\" PGRgenesys <- read.genesys(\"genesys-accessions-filtered.zip\",                            scrub.names.space = TRUE, readme = TRUE)"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"data-pre-processing","dir":"Articles","previous_headings":"","what":"Data Pre-processing","title":"An Introduction to `PGRdup` Package","text":"Data pre-processing critical step can affect quality probable duplicate sets retrieved. involves data standardization well data cleaning can achieved using functions DataClean, MergeKW, MergePrefix MergeSuffix. DataClean function can used clean character strings passport data fields(columns) specified input character vector x according conditions specified arguments. Commas, semicolons colons sometimes used separate multiple strings names within field can replaced single space using logical arguments fix.comma, fix.semcol fix.col respectively. Similarly logical argument fix.bracket can used replace brackets including parenthesis, square brackets curly brackets space. logical argument fix.punct can used remove punctuation data. fix.space can used convert space characters tab, newline, vertical tab, form feed carriage return spaces finally convert multiple spaces single space. fix.sep can used merge together accession identifiers composed alphabetic characters separated series digits space character. fix.leadzero can used remove leading zeros accession name fields facilitate matching identify probable duplicates. function can hence made use tidying multiple forms messy data existing fields associated accession names PGR passport databases (Table 1). Table 2. Data pre-processing using DataClean. Several common keyword string pairs keyword prefixes suffixes exist fields associated accession names PGR passport databases. can merged using functions MergeKW, MergePrefix MergeSuffix respectively. keyword string pairs, prefixes suffixes can supplied list vector argument y functions. functions can applied multiple columns(fields) data frame using lapply function.","code":"x <- c(\"A 14; EC 1697\", \"U 4-4-28; EC 21078; A 32\", \"PI 262801:CIAT 9075:GKP 9553/90\",        \"NCAC 16049, PI 261987, RCM 493-3\") x [1] \"A 14; EC 1697\"                    \"U 4-4-28; EC 21078; A 32\"         [3] \"PI 262801:CIAT 9075:GKP 9553/90\"  \"NCAC 16049, PI 261987, RCM 493-3\" # Replace ',', ':' and ';' with space DataClean(x, fix.comma=TRUE, fix.semcol=TRUE, fix.col=TRUE,           fix.bracket=FALSE, fix.punct=FALSE, fix.space=FALSE, fix.sep=FALSE,           fix.leadzero=FALSE) [1] \"A 14  EC 1697\"                    \"U 4-4-28  EC 21078  A 32\"         [3] \"PI 262801 CIAT 9075 GKP 9553/90\"  \"NCAC 16049  PI 261987  RCM 493-3\" x <- c(\"(NRCG-1738)/(NFG649)\", \"26-5-1[NRCG-2528]\", \"Ah 1182 {NRCG-4340}\") x [1] \"(NRCG-1738)/(NFG649)\" \"26-5-1[NRCG-2528]\"    \"Ah 1182 {NRCG-4340}\" # Replace parenthesis, square brackets and curly brackets with space DataClean(x, fix.comma=FALSE, fix.semcol=FALSE, fix.col=FALSE,           fix.bracket=TRUE,           fix.punct=FALSE, fix.space=FALSE, fix.sep=FALSE, fix.leadzero=FALSE) [1] \"NRCG-1738 / NFG649\" \"26-5-1 NRCG-2528\"   \"AH 1182  NRCG-4340\" x <- c(\"#26-6-3-1\", \"Culture No. 857\", \"U/4/47/13\") x [1] \"#26-6-3-1\"       \"Culture No. 857\" \"U/4/47/13\" # Remove punctuation DataClean(x, fix.comma=FALSE, fix.semcol=FALSE, fix.col=FALSE, fix.bracket=FALSE,           fix.punct=TRUE,           fix.space=FALSE, fix.sep=FALSE, fix.leadzero=FALSE) [1] \"26631\"          \"CULTURE NO 857\" \"U44713\" x <- c(\"RS   1\", \"GKSPScGb 208  PI 475855\") x [1] \"RS   1\"                  \"GKSPScGb 208  PI 475855\" # Replace all space characters to space and convert multiple spaces to single space DataClean(x, fix.comma=FALSE, fix.semcol=FALSE, fix.col=FALSE,           fix.bracket=FALSE, fix.punct=FALSE,           fix.space=TRUE,           fix.sep=FALSE, fix.leadzero=FALSE) [1] \"RS 1\"                   \"GKSPSCGB 208 PI 475855\" x <- c(\"NCAC 18078\", \"AH 6481\", \"ICG 2791\") x [1] \"NCAC 18078\" \"AH 6481\"    \"ICG 2791\" # Merge alphabetic character separated from a series of digits by a space DataClean(x, fix.comma=FALSE, fix.semcol=FALSE, fix.col=FALSE,           fix.bracket=FALSE, fix.punct=FALSE, fix.space=FALSE,           fix.sep=TRUE,           fix.leadzero=FALSE) [1] \"NCAC18078\" \"AH6481\"    \"ICG2791\" x <- c(\"EC 0016664\", \"EC0001690\") x [1] \"EC 0016664\" \"EC0001690\" # Remove leading zeros DataClean(x, fix.comma=FALSE, fix.semcol=FALSE, fix.col=FALSE,           fix.bracket=FALSE, fix.punct=FALSE, fix.space=FALSE, fix.sep=FALSE,           fix.leadzero=TRUE) [1] \"EC 16664\" \"EC1690\" names <- c(\"S7-12-6\", \"ICG-3505\", \"U 4-47-18;EC 21127\", \"AH 6481\", \"RS   1\",            \"AK 12-24\", \"2-5 (NRCG-4053)\", \"T78, Mwitunde\", \"ICG 3410\",            \"#648-4 (Gwalior)\", \"TG4;U/4/47/13\", \"EC0021003\") names [1] \"S7-12-6\"            \"ICG-3505\"           \"U 4-47-18;EC 21127\"  [4] \"AH 6481\"            \"RS   1\"             \"AK 12-24\"            [7] \"2-5 (NRCG-4053)\"    \"T78, Mwitunde\"      \"ICG 3410\"           [10] \"#648-4 (Gwalior)\"   \"TG4;U/4/47/13\"      \"EC0021003\" # Clean the data DataClean(names) [1] \"S7126\"          \"ICG3505\"        \"U44718 EC21127\" \"AH6481\"          [5] \"RS1\"            \"AK1224\"         \"25 NRCG4053\"    \"T78 MWITUNDE\"    [9] \"ICG3410\"        \"6484 GWALIOR\"   \"TG4 U44713\"     \"EC21003\" names <- c(\"Punjab Bold\", \"Gujarat- Dwarf\", \"Nagpur.local\", \"SAM COL 144\",            \"SAM COL--280\", \"NIZAMABAD-LOCAL\", \"Dark Green Mutant\",            \"Dixie-Giant\", \"Georgia- Bunch\", \"Uganda-erect\", \"Small Japan\",            \"Castle  Cary\", \"Punjab erect\", \"Improved small japan\",            \"Dark Purple\") names [1] \"Punjab Bold\"          \"Gujarat- Dwarf\"       \"Nagpur.local\"          [4] \"SAM COL 144\"          \"SAM COL--280\"         \"NIZAMABAD-LOCAL\"       [7] \"Dark Green Mutant\"    \"Dixie-Giant\"          \"Georgia- Bunch\"       [10] \"Uganda-erect\"         \"Small Japan\"          \"Castle  Cary\"         [13] \"Punjab erect\"         \"Improved small japan\" \"Dark Purple\" # Merge pairs of strings y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"),            c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"),            c(\"Mota\", \"Company\")) names <- MergeKW(names, y1, delim = c(\"space\", \"dash\", \"period\"))  # Merge prefix strings y2 <- c(\"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\", \"Dark\") names <- MergePrefix(names, y2, delim = c(\"space\", \"dash\", \"period\"))  # Merge suffix strings y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") names <- MergeSuffix(names, y3, delim = c(\"space\", \"dash\", \"period\"))  names [1] \"PunjabBold\"         \"GujaratDwarf\"       \"Nagpurlocal\"         [4] \"SAMCOL 144\"         \"SAMCOL--280\"        \"NIZAMABADLOCAL\"      [7] \"DarkGreenMutant\"    \"DixieGiant\"         \"GeorgiaBunch\"       [10] \"Ugandaerect\"        \"SmallJapan\"         \"CastleCary\"         [13] \"Punjaberect\"        \"Improvedsmalljapan\" \"DarkPurple\" # Load example dataset GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\") head(GN[GNfields]) NationalID                CollNo   DonorID OtherID1  OtherID2 1   EC100277 Shulamith/ NRCG-14555  ICG-4709           U4-47-12 2   EC100280                    NC   ICG5288      NCS      NC 5 3   EC100281               MALIMBA   ICG5289          EC 100281 4   EC100713            EC 100713;   ICG5296              STARR 5   EC100715             EC 100715   ICG5298              COMET 6   EC100716                        ICG-3150          ARGENTINE # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\"))) head(GN[GNfields]) NationalID              CollNo DonorID OtherID1  OtherID2 1   EC100277 SHULAMITH NRCG14555 ICG4709             U44712 2   EC100280                  NC ICG5288      NCS       NC5 3   EC100281             MALIMBA ICG5289           EC100281 4   EC100713            EC100713 ICG5296              STARR 5   EC100715            EC100715 ICG5298              COMET 6   EC100716                     ICG3150          ARGENTINE"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"generation-of-kwic-index","dir":"Articles","previous_headings":"","what":"Generation of KWIC Index","title":"An Introduction to `PGRdup` Package","text":"function KWIC generates Key Word Context index (Knüpffer 1988; Knüpffer, Frese, Jongen 1997) data frame PGR passport database based fields(columns) specified argument fields along keyword frequencies gives output list class KWIC. first element vector specified fields considered primary key identifier uniquely identifies rows data frame. function fetches keywords different fields specified, can subsequently used matching identify probable duplicates. frequencies keywords retrieved can help determining data pre-processing required also decide whether common keywords can exempted matching (Fig. 2).  Fig. 2. Word cloud keywords retrieved function throw error case duplicates NULL values primary key/ID field mentioned. erroneous records can identified using helper function ValidatePrimKey.","code":"# Load example dataset GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate the KWIC index GNKWIC <- KWIC(GN, GNfields, min.freq = 1) class(GNKWIC) [1] \"KWIC\" GNKWIC KWIC fields : NationalID CollNo DonorID OtherID1 OtherID2 Number of keywords : 3893 Number of distinct keywords : 3109 # Retrieve the KWIC index from the KWIC object KWIC <- GNKWIC[[1]] KWIC <- KWIC[order(KWIC$KEYWORD, decreasing = TRUE),] head(KWIC[,c(\"PRIM_ID\", \"KWIC_L\", \"KWIC_KW\", \"KWIC_R\")], n = 10) PRIM_ID                                     KWIC_L  KWIC_KW 550  EC490380            EC490380 =  = ICG1122 =  = LIN      YUCH 435   EC36893                                 EC36893 =      YUAN 434   EC36893                            EC36893 = YUAN     YOUNG 1287 EC613524       EC613524 = NRCG9225 =  = PEI KANGPE    YOUDON 1703 IC113088                       IC113088 =  =  = SB        XI 1741 IC296965 IC296965 = SB X11 X V11 = ICG1769 =  = SB        XI 3385 IC445197                                IC445197 =   X144B28 3483 IC494754                IC494754 =  = ICG7686 =  =   X144B28 2090 IC304018    IC304018 = 144B19B NRCG = ICG1561 =  =  X144B19B 1735 IC296965                             IC296965 = SB       X11                                 KWIC_R 550                               TSAO 435   YOUNG TOU = ICG5241 =  = EC36893 434         TOU = ICG5241 =  = EC36893 1287                                 = 1703                        = IC305003 1741                             X VII 3385           B = ICG2113 =  = LIMDI4 3483                                 B 2090                                   1735  X V11 = ICG1769 =  = SB XI X VII # Retrieve the keyword frequencies from the KWIC object KeywordFreq <- GNKWIC[[2]] head(KeywordFreq) Keyword Freq 1   OVERO   25 2      S1   19 3       A   11 4     RED   11 5    OVER   10 6  PURPLE   10 GN <- GN1000 GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) # Generate dummy duplicates for illustration GN[1001:1005,] <- GN[1:5,] # Generate dummy NULL values for illustration GN[1001,3] <- \"\" GN[1002,3] <- \"\" GN[1001:1005,] CommonName    BotanicalName NationalID              CollNo DonorID 1001  Groundnut Arachis hypogaea            SHULAMITH NRCG14555 ICG4709 1002  Groundnut Arachis hypogaea                             NC ICG5288 1003  Groundnut Arachis hypogaea   EC100281             MALIMBA ICG5289 1004  Groundnut Arachis hypogaea   EC100713            EC100713 ICG5296 1005  Groundnut Arachis hypogaea   EC100715            EC100715 ICG5298      OtherID1 OtherID2 BioStatus            SourceCountry TransferYear 1001            U44712  Landrace                   Israel         2014 1002      NCS      NC5  Landrace United States of America         2004 1003          EC100281  Landrace                   Malawi         2004 1004             STARR  Landrace United States of America         2004 1005             COMET  Landrace United States of America         2004 GNKWIC <- KWIC(GN, GNfields, min.freq=1) Error in KWIC(GN, GNfields, min.freq = 1) :   Primary key/ID field should be unique and not NULL  Use PGRdup::ValidatePrimKey() to identify and rectify the aberrant records first # Validate the primary key/ID field for duplication or existence of NULL values ValidatePrimKey(x = GN, prim.key = \"NationalID\") $message1 [1] \"ERROR: Duplicated records found in prim.key field\"  $Duplicates      CommonName    BotanicalName NationalID              CollNo DonorID 1001  Groundnut Arachis hypogaea            SHULAMITH NRCG14555 ICG4709 1002  Groundnut Arachis hypogaea                             NC ICG5288 3     Groundnut Arachis hypogaea   EC100281             MALIMBA ICG5289 1003  Groundnut Arachis hypogaea   EC100281             MALIMBA ICG5289 4     Groundnut Arachis hypogaea   EC100713            EC100713 ICG5296 1004  Groundnut Arachis hypogaea   EC100713            EC100713 ICG5296 5     Groundnut Arachis hypogaea   EC100715            EC100715 ICG5298 1005  Groundnut Arachis hypogaea   EC100715            EC100715 ICG5298      OtherID1 OtherID2 BioStatus            SourceCountry TransferYear 1001            U44712  Landrace                   Israel         2014 1002      NCS      NC5  Landrace United States of America         2004 3             EC100281  Landrace                   Malawi         2004 1003          EC100281  Landrace                   Malawi         2004 4                STARR  Landrace United States of America         2004 1004             STARR  Landrace United States of America         2004 5                COMET  Landrace United States of America         2004 1005             COMET  Landrace United States of America         2004  $message2 [1] \"ERROR: NULL records found in prim.key field\"  $NullRecords      CommonName    BotanicalName NationalID              CollNo DonorID 1001  Groundnut Arachis hypogaea            SHULAMITH NRCG14555 ICG4709 1002  Groundnut Arachis hypogaea                             NC ICG5288      OtherID1 OtherID2 BioStatus            SourceCountry TransferYear primdup 1001            U44712  Landrace                   Israel         2014    TRUE 1002      NCS      NC5  Landrace United States of America         2004    TRUE # Remove the offending records GN <- GN[-c(1001:1005), ] # Validate again ValidatePrimKey(x = GN, prim.key = \"NationalID\") $message1 [1] \"OK: No duplicated records found in prim.key field\"  $Duplicates NULL  $message2 [1] \"OK: No NULL records found in prim.key field\"  $NullRecords NULL"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"retrieval-of-probable-duplicate-sets","dir":"Articles","previous_headings":"","what":"Retrieval of Probable Duplicate Sets","title":"An Introduction to `PGRdup` Package","text":"KWIC indexes generated, probable duplicates germplasm accessions can identified fuzzy, phonetic semantic matching associated keywords using function ProbDup. sets retrieved list data frames class ProbDup. Keywords used matching can specified vector excep argument.","code":""},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"methods","dir":"Articles","previous_headings":"Retrieval of Probable Duplicate Sets","what":"Methods","title":"An Introduction to `PGRdup` Package","text":"function can execute matching according either one following three methods specified method argument. Method \"\" : Performs string matching keywords single KWIC index identify probable duplicates accessions single PGR passport database. Method \"b\" : Performs string matching keywords first KWIC index (query) keywords second index (source) identify probable duplicates accessions first PGR passport database among accessions second database. Method \"c\" : Performs string matching keywords two different KWIC indexes jointly identify probable duplicates accessions among two PGR passport databases.","code":"# Load example dataset GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate the KWIC index GNKWIC <- KWIC(GN, GNfields) # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Fetch fuzzy duplicates by method 'a' GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = FALSE, semantic = FALSE) Fuzzy matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | class(GNdup) [1] \"ProbDup\" GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                   No..of.Sets    No..of.Records FuzzyDuplicates         378               745 Total                   378 745(Distinct:745) # Fetch phonetic duplicates by method 'a' GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = FALSE,                  phonetic = TRUE, semantic = FALSE) Phonetic matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | class(GNdup) [1] \"ProbDup\" GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets    No..of.Records PhoneticDuplicates          99               260 Total                       99 260(Distinct:260) # Load PGR passport databases GN1 <- GN1000[!grepl(\"^ICG\", GN1000$DonorID), ] GN1$DonorID <- NULL GN2 <- GN1000[grepl(\"^ICG\", GN1000$DonorID), ] GN2$NationalID <- NULL  # Specify database fields to use GN1fields <- c(\"NationalID\", \"CollNo\", \"OtherID1\", \"OtherID2\") GN2fields <- c(\"DonorID\", \"CollNo\", \"OtherID1\", \"OtherID2\")  # Clean the data GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) DataClean(x)) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN1[GN1fields] <- lapply(GN1[GN1fields],                          function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields],                          function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields],                          function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields],                          function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields],                          function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields],                          function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Remove duplicated DonorID records in GN2 GN2 <- GN2[!duplicated(GN2$DonorID), ]  # Generate KWIC index GN1KWIC <- KWIC(GN1, GN1fields) GN2KWIC <- KWIC(GN2, GN2fields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Fetch fuzzy and phonetic duplicate sets by method b GNdupb <- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = \"b\",                   excep = exep, fuzzy = TRUE, phonetic = TRUE,                   encoding = \"primary\", semantic = FALSE) Fuzzy matching |                                                                              |======================================================================| 100%Block 1 / 1 | Phonetic matching |                                                                              |======================================================================| 100%Block 1 / 1 | class(GNdupb) [1] \"ProbDup\" GNdupb Method : b  KWIC1 fields : NationalID CollNo OtherID1 OtherID2  KWIC2 fields : DonorID CollNo OtherID1 OtherID2                      No..of.Sets    No..of.Records FuzzyDuplicates            107               353 PhoneticDuplicates          41               126 Total                      148 479(Distinct:383) # Fetch fuzzy and phonetic duplicate sets by method c GNdupc <- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = \"c\",                   excep = exep, fuzzy = TRUE, phonetic = TRUE,                   encoding = \"primary\", semantic = FALSE) Fuzzy matching |                                                                              |=======================                                               |  33%Block 1 / 3 |  |                                                                              |===============================================                       |  67%Block 2 / 3 |  |                                                                              |======================================================================| 100%Block 3 / 3 | Phonetic matching |                                                                              |=======================                                               |  33%Block 1 / 3 |  |                                                                              |===============================================                       |  67%Block 2 / 3 |  |                                                                              |======================================================================| 100%Block 3 / 3 | class(GNdupc) [1] \"ProbDup\" GNdupc Method : c  KWIC1 fields : NationalID CollNo OtherID1 OtherID2  KWIC2 fields : DonorID CollNo OtherID1 OtherID2                      No..of.Sets    No..of.Records FuzzyDuplicates            363               724 PhoneticDuplicates          98               257 Total                      461 981(Distinct:741)"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"matching-strategies","dir":"Articles","previous_headings":"Retrieval of Probable Duplicate Sets","what":"Matching Strategies","title":"An Introduction to `PGRdup` Package","text":"Fuzzy matching approximate string matching keywords carried computing generalized levenshtein (edit) distance . distance measure counts number deletions, insertions substitutions necessary turn one string another. maximum distance considered match can specified max.dist argument. Exact matching can enforced argument force.exact set TRUE. can used avoid fuzzy matching number alphabet characters keywords lesser critical value (max.alpha). Similarly, value max.digit can also set according requirements enforce exact matching. default value Inf avoids fuzzy matching enforces exact matching keywords numerical characters. max.digit max.alpha set Inf, exact matching enforced keywords. exact matching enforced, keywords alphabet numeric characters number alphabet characters greater max.digit, matching carried separately alphabet numeric characters present. Phonetic matching keywords carried using Double Metaphone phonetic algorithm implemented helper function DoubleMetaphone, (Philips 2000), identify keywords similar pronunciation. Either primary alternate encodings can used specifying encoding argument. argument phon.min.alpha sets limits number alphabet characters present string executing phonetic matching. Similarly min.enc sets limits number characters present encoding keyword phonetic matching. Semantic matching matches keywords based list accession name synonyms supplied list character vectors synonym sets (synsets) syn argument. Synonyms context refer interchangeable identifiers names accession recognized. Multiple keywords specified members synset syn matched. facilitate accurate identification synonyms KWIC index, identical data standardization operations using Merge* DataClean functions original database fields synset list recommended.","code":"# Load example dataset GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate the KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Fetch fuzzy duplicates GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = TRUE, max.dist = 3,                  phonetic = FALSE, semantic = FALSE) Fuzzy matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                   No..of.Sets    No..of.Records FuzzyDuplicates         378               745 Total                   378 745(Distinct:745) GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = TRUE, max.dist = 1,                  phonetic = FALSE, semantic = FALSE) Fuzzy matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                   No..of.Sets    No..of.Records FuzzyDuplicates         288               679 Total                   288 679(Distinct:679) GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = TRUE, force.exact = TRUE, max.alpha = 4, max.digit = Inf,                  phonetic = FALSE, semantic = FALSE) Fuzzy matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                   No..of.Sets    No..of.Records FuzzyDuplicates         378               745 Total                   378 745(Distinct:745) GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = FALSE,                  phonetic = TRUE,                  semantic = FALSE) Phonetic matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets    No..of.Records PhoneticDuplicates          99               260 Total                       99 260(Distinct:260) GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = FALSE,                  phonetic = TRUE, encoding = \"alternate\",                  semantic = FALSE) Phonetic matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets    No..of.Records PhoneticDuplicates          98               263 Total                       98 263(Distinct:263) GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = FALSE,                  phonetic = TRUE, encoding = \"alternate\", phon.min.alpha = 4,                  semantic = FALSE) Phonetic matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets    No..of.Records PhoneticDuplicates         304               451 Total                      304 451(Distinct:451) GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = FALSE,                  phonetic = TRUE, encoding = \"alternate\", min.enc = 4,                  semantic = FALSE) Phonetic matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets    No..of.Records PhoneticDuplicates          59               156 Total                       59 156(Distinct:156) # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH 114\"), c(\"TG-1\", \"VIKRAM\"))  # Clean the data in the synsets syn <- lapply(syn, DataClean)  GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                   fuzzy = FALSE, phonetic = FALSE,                  semantic = TRUE, syn = syn) Semantic matching |                                                                              |==================                                                    |  25%Block 1 / 4 |  |                                                                              |===================================                                   |  50%Block 2 / 4 |  |                                                                              |====================================================                  |  75%Block 3 / 4 |  |                                                                              |======================================================================| 100%Block 4 / 4 | GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets No..of.Records SemanticDuplicates           2              5 Total                        2  5(Distinct:5)"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"memory-and-speed-constraints","dir":"Articles","previous_headings":"Retrieval of Probable Duplicate Sets","what":"Memory and Speed Constraints","title":"An Introduction to `PGRdup` Package","text":"number keywords KWIC indexes increases, memory consumption function also increases proportionally. due reason string matching, function relies upon creation n\\(\\times\\)m matrix possible keyword pairs comparison, n m number keywords query source indexes respectively. can lead allocate vector size... errors case large KWIC indexes comparison matrix large reside memory. case, chunksize argument can reduced default 1000 get appropriate size KWIC index keyword block used searching matches time. However smaller chunksize may lead longer computation time due memory-time trade-. progress matching displayed console number keyword blocks completed total number blocks, percentage achievement text-based progress bar. case multi-byte characters keywords, speed keyword matching dependent upon useBytes argument described help(\"stringdist-encoding\") stringdist function namesake package (Loo 2014), made use string matching. CPU time taken retrieval probable duplicate sets different options arguments chunksize useBytes can visualized using microbenchmark package (Fig. 3).  Fig. 3. CPU time different ProbDup arguments estimated using microbenchmark package.","code":"# Load example dataset GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"),            c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"),            c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\", \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate the KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\", \"DARK\",           \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\", \"GUTHUKAI\",           \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\", \"LIGHT\", \"LOCAL\",           \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\", \"RED\", \"RUNNER\", \"S1\", \"SAM\",           \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\", \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH 114\"), c(\"TG-1\", \"VIKRAM\")) syn <- lapply(syn, DataClean) timings <- microbenchmark::microbenchmark(   # Fetch duplicate sets with default chunk.size   t1 = ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                                      chunksize = 1000, useBytes = TRUE,                                      fuzzy = TRUE, phonetic = TRUE,                                      semantic = TRUE, syn = syn),   # Fetch duplicate sets chunk.size 2000   t2 = ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                                      chunksize = 2000, useBytes = TRUE,                                      fuzzy = TRUE, phonetic = TRUE,                                      semantic = TRUE, syn = syn),   # Fetch duplicate sets chunk.size 100   t3 = ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                                      chunksize = 100, useBytes = TRUE,                                      fuzzy = TRUE, phonetic = TRUE,                                      semantic = TRUE, syn = syn),   # Fetch duplicate sets useBytes = FALSE   t4 = ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep,                                      chunksize = 1000, useBytes = FALSE,                                      fuzzy = TRUE, phonetic = TRUE,                                      semantic = TRUE, syn = syn), times = 10) plot(timings, col = c(\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\"),      xlab = \"Expression\", ylab = \"Time\") legend(\"topright\", c(\"t1 : chunksize = 1000,\\n     useBytes = T (default)\\n\",          \"t2 : chunksize = 2000,\\n     useBytes = T\\n\",          \"t3 : chunksize = 500,\\n     useBytes = T\\n\",          \"t4 : chunksize = 1000,\\n     useBytes = F\\n\"),        bty = \"n\", cex = 0.6)"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"set-review-modification-and-validation","dir":"Articles","previous_headings":"","what":"Set Review, Modification and Validation","title":"An Introduction to `PGRdup` Package","text":"initially retrieved sets may intersecting might accessions occur duplicate set. Disjoint sets can generated merging overlapping sets using function DisProbDup. Disjoint sets retrieved either individually type probable duplicate sets considering type sets simultaneously. case latter, disjoint type sets alone returned output additional data frame DisjointDupicates object class ProbDup. duplicate sets retrieved can validated manual clerical review comparing original PGR passport database(s) using ReviewProbDup function. function helps retrieve PGR passport information associated fuzzy, phonetic semantic probable duplicate sets object class ProbDup original databases(s) identified. original information accessions comprising set, subjected data standardization can compared manual clerical review validation set. default fields(columns) used initially creation KWIC indexes using KWIC function retrieved. Additional fields(columns) necessary can specified using extra.db1 extra.db2 arguments. primary ID/key records fuzzy, phonetic semantic duplicate sets found missing original databases specified db1 db2, ignored matching records considered retrieving information warning. may due data standardization primary ID/key field using function DataClean creation KWIC index subsequent identification probable duplicate sets. case, recommended use identical data standardization operation primary ID/key field databases specified db1 db2 running function. R <= v3.0.2, due copying named objects list(), Invalid .internal.selfref detected fixed... warning can appear, may safely ignored. output data frame can subjected clerical review either exporting external spreadsheet using write.csv function using edit function. column DEL can used indicate whether record deleted set . Y indicates “Yes”, default N indicates “”. column SPLIT similarly can used indicate whether record set branched new set. set identical integers column default 0 can used indicate removed assembled new set. clerical review, data frame created using function ReviewProbDup object class ProbDup can reconstituted back object review using function ReconstructProbDup. instructions modifying sets entered appropriate format columns DEL SPLIT clerical review taken account reconstituting probable duplicate sets. records Y column DEL deleted records identical integers column SPLIT default 0 reassembled new set.","code":"# Load example dataset GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                  semantic = TRUE, syn = syn) # Initial number of sets GNdup Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets     No..of.Records FuzzyDuplicates            378                745 PhoneticDuplicates          99                260 SemanticDuplicates           2                  5 Total                      479 1010(Distinct:762) # Get disjoint probable duplicate sets of each kind disGNdup1 <- DisProbDup(GNdup, combine = NULL) # # Number of sets after combining intersecting sets disGNdup1 Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets     No..of.Records FuzzyDuplicates            181                745 PhoneticDuplicates          80                260 SemanticDuplicates           2                  5 Total                      263 1010(Distinct:762) # Get disjoint probable duplicate sets combining all the kinds of sets disGNdup2 <- DisProbDup(GNdup, combine = c(\"F\", \"P\", \"S\")) # Number of sets after combining intersecting sets disGNdup2 Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                     No..of.Sets    No..of.Records DisjointDupicates         167               762 Total                     167 762(Distinct:762) # Load the original database and clean the Primary ID/key field GN1000 <- GN1000 GN1000$NationalID <- DataClean(GN1000$NationalID)  # Get the data frame for reviewing the duplicate sets identified RevGNdup <- ReviewProbDup(pdup = disGNdup1, db1 = GN1000,                           extra.db1 = c(\"SourceCountry\", \"TransferYear\"),                           max.count = 30, insert.blanks = TRUE) head(RevGNdup) SET_NO TYPE K[a]  PRIM_ID                IDKW  DEL SPLIT COUNT K1_NationalID 1      1    F [K1] EC100277 [K1]EC100277:U44712    N     0     3      EC100277 2      1    F [K1]  EC21118  [K1]EC21118:U44712    N     0     3       EC21118 3      1    F [K1] IC494796 [K1]IC494796:U44712    N     0     3      IC494796 4     NA      <NA>     <NA>                <NA> <NA>    NA    NA          <NA> 5      1    P [K1] EC100713  [K1]EC100713:STARR    N     0    14      EC100713 6      1    P [K1] EC106985  [K1]EC106985:STARR    N     0    14      EC106985                  K1_CollNo K1_DonorID K1_OtherID1  K1_OtherID2 1    Shulamith/ NRCG-14555   ICG-4709                 U4-47-12 2 U 4-47-12; EC 21118; UKA    ICG3265             U44712 U K A 3                U-4-47-12   ICG-6890                   U44712 4                     <NA>       <NA>        <NA>         <NA> 5               EC 100713;    ICG5296                    STARR 6                    Starr    ICG3479                                   K1X_SourceCountry K1X_TransferYear 1                   Israel             2014 2                Australia             1989 3                  Unknown             2010 4                     <NA>               NA 5 United States of America             2004 6 United States of America             2001 # Examine and review the duplicate sets using edit function RevGNdup <- edit(RevGNdup)  # OR examine and review the duplicate sets after exporting them as a csv file write.csv(file=\"Duplicate sets for review.csv\", x=RevGNdup) # The original set data subset(RevGNdup, SET_NO==13 & TYPE==\"P\", select= c(IDKW, DEL, SPLIT)) IDKW DEL SPLIT 111                         [K1]EC38607:MANFREDI1   N     0 112                         [K1]EC420966:MANFREDI   N     0 113                        [K1]EC42549:MANFREDI68   N     0 114                          [K1]EC42550:MANFRED1   N     0 115 [K1]EC552714:CHAMPAQUI, [K1]EC552714:MANFREDI   N     0 116                       [K1]EC573128:MANFREDI84   N     0 117 [K1]IC304523:CHAMPAGUE, [K1]IC304523:MANFREDI   N     0 # Make dummy changes to the set for illustration RevGNdup[c(113, 116), 6] <- \"Y\" RevGNdup[c(111, 114), 7] <- 1 RevGNdup[c(112, 115, 117), 7] <- 2 # The instruction for modification in columns DEL and SPLIT subset(RevGNdup, SET_NO==13 & TYPE==\"P\", select= c(IDKW, DEL, SPLIT)) IDKW DEL SPLIT 111                         [K1]EC38607:MANFREDI1   N     1 112                         [K1]EC420966:MANFREDI   N     2 113                        [K1]EC42549:MANFREDI68   Y     0 114                          [K1]EC42550:MANFRED1   N     1 115 [K1]EC552714:CHAMPAQUI, [K1]EC552714:MANFREDI   N     2 116                       [K1]EC573128:MANFREDI84   Y     0 117 [K1]IC304523:CHAMPAGUE, [K1]IC304523:MANFREDI   N     2 # Reconstruct ProDup object GNdup2 <- ReconstructProbDup(RevGNdup) # Initial no. of sets disGNdup1 Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets     No..of.Records FuzzyDuplicates            181                745 PhoneticDuplicates          80                260 SemanticDuplicates           2                  5 Total                      263 1010(Distinct:762) # No. of sets after modifications GNdup2 Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets    No..of.Records FuzzyDuplicates            180               523 PhoneticDuplicates          81               258 SemanticDuplicates           2                 5 Total                      263 786(Distinct:674)"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"other-functions","dir":"Articles","previous_headings":"","what":"Other Functions","title":"An Introduction to `PGRdup` Package","text":"ProbDup object list data frames different kinds probable duplicate sets viz- FuzzyDuplicates, PhoneticDuplicates, SemanticDuplicates DisjointDuplicates. row component data frame information set, type set, set members well keywords based set formed. data can reshaped long form using function ParseProbDup. function transform ProbDup object single data frame. prefix K* indicates KWIC index origin. useful ascertaining database origin accessions method \"b\" \"c\" used create input ProbDup object. sets reviewed modified, validated set data fields ProbDup object can added original PGR passport database using function AddProbDup. associated data fields SET_NO, ID IDKW added based PRIM_ID field(column). case one KWIC index used generate object class ProbDup, argument addto can used specify database data fields added. default \"\" indicates database first KWIC index created \"II\" indicates database second index created. function SplitProbDup can used split object class ProbDup two basis set counts. useful reviewing separately sets larger set counts. Alternatively, two different ProbDup objects can merged together using function MergeProbDup. summary accessions according grouping factor field(column) original database(s) within probable duplicate sets retrieved ProbDup object can visualized ViewProbDup function. resulting plot can used examine extent probable duplication within groups accessions records.  Fig. 5. Summary visualization groundnut probable duplicate sets retrieved according SourceCountry field. function KWCounts can used compute keyword counts PGR passport database fields(columns) considered identification probable duplicates. keyword counts can give rough indication completeness data fields (Fig. 3).  Fig. 6. keyword counts database fields considered identification probable duplicates . entire GN1000 dataset, B. probable duplicate records alone C. unique records alone.","code":"# Convert 'ProbDup' object to a long form data frame of sets GNdupParsed <- ParseProbDup(GNdup) head(GNdupParsed) SET_NO TYPE    K  PRIM_ID                IDKW COUNT 1      1    F [K1] EC100277 [K1]EC100277:U44712     3 2      1    F [K1]  EC21118  [K1]EC21118:U44712     3 3      1    F [K1] IC494796 [K1]IC494796:U44712     3 4     NA      <NA>     <NA>                <NA>    NA 5      2    F [K1] EC100280    [K1]EC100280:NC5     3 6      2    F [K1] EC100721    [K1]EC100721:NC5     3 # Loading original database GN2 <- GN1000  # Add the duplicates set data to the original database GNwithdup <-  AddProbDup(pdup = GNdup, db = GN2, addto = \"I\") # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields],                        function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                  semantic = TRUE, syn = syn) # Split the probable duplicate sets GNdupSplit <- SplitProbDup(GNdup, splitat = c(10, 10, 10)) GNdupSplit[[1]] Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets     No..of.Records FuzzyDuplicates            338                744 PhoneticDuplicates          99                260 SemanticDuplicates           2                  5 Total                      439 1009(Distinct:762) GNdupSplit[[3]] Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                   No..of.Sets    No..of.Records FuzzyDuplicates          40               136 Total                    40 136(Distinct:136) GNdupMerged <- MergeProbDup(GNdupSplit[[1]], GNdupSplit[[3]]) GNdupMerged Method : a  KWIC1 fields : NationalID CollNo DonorID OtherID1 OtherID2                      No..of.Sets     No..of.Records FuzzyDuplicates            378                745 PhoneticDuplicates          99                260 SemanticDuplicates           2                  5 Total                      479 1010(Distinct:762) # Load PGR passport databases GN1 <- GN1000[!grepl(\"^ICG\", GN1000$DonorID), ] GN1$DonorID <- NULL GN2 <- GN1000[grepl(\"^ICG\", GN1000$DonorID), ] GN2 <- GN2[!grepl(\"S\", GN2$DonorID), ] GN2$NationalID <- NULL  GN1$SourceCountry <- toupper(GN1$SourceCountry) GN2$SourceCountry <- toupper(GN2$SourceCountry)  GN1$SourceCountry <- gsub(\"UNITED STATES OF AMERICA\", \"USA\", GN1$SourceCountry) GN2$SourceCountry <- gsub(\"UNITED STATES OF AMERICA\", \"USA\", GN2$SourceCountry)  # Specify as a vector the database fields to be used GN1fields <- c(\"NationalID\", \"CollNo\", \"OtherID1\", \"OtherID2\") GN2fields <- c(\"DonorID\", \"CollNo\", \"OtherID1\", \"OtherID2\")  # Clean the data GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) DataClean(x)) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"),            c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"),            c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN1[GN1fields] <- lapply(GN1[GN1fields],                          function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields],                          function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields],                          function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields],                          function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields],                          function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields],                          function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Remove duplicated DonorID records in GN2 GN2 <- GN2[!duplicated(GN2$DonorID), ]  # Generate KWIC index GN1KWIC <- KWIC(GN1, GN1fields) GN2KWIC <- KWIC(GN2, GN2fields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\")) GNdupc <- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = \"c\",                   excep = exep, fuzzy = TRUE, phonetic = TRUE,                   encoding = \"primary\", semantic = TRUE, syn = syn) Fuzzy matching |                                                                              |=======================                                               |  33%Block 1 / 3 |  |                                                                              |===============================================                       |  67%Block 2 / 3 |  |                                                                              |======================================================================| 100%Block 3 / 3 | Phonetic matching |                                                                              |=======================                                               |  33%Block 1 / 3 |  |                                                                              |===============================================                       |  67%Block 2 / 3 |  |                                                                              |======================================================================| 100%Block 3 / 3 | Semantic matching |                                                                              |=======================                                               |  33%Block 1 / 3 |  |                                                                              |===============================================                       |  67%Block 2 / 3 |  |                                                                              |======================================================================| 100%Block 3 / 3 | # Get the summary data.frames and Grob GNdupcView <- ViewProbDup(GNdupc, GN1, GN2, \"SourceCountry\", \"SourceCountry\",                          max.count = 30, select = c(\"INDIA\", \"USA\"), order = \"type\",                          main = \"Groundnut Probable Duplicates\") Warning:  [1m [22mThe `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as of ggplot2 3.3.4.  [36mℹ [39m The deprecated feature was likely used in the  [34mPGRdup [39m package.   Please report the issue at  [3m [34m<https://github.com/aravind-j/PGRdup/issues> [39m [23m. This warning is displayed once every 8 hours. Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. # View the summary data.frames GNdupcView[[1]] GNdupcView[[2]] # Plot the summary visualization library(gridExtra) grid.arrange(GNdupcView[[3]]) # Compute the keyword counts for the whole data GNKWCouts <- KWCounts(GN, GNfields, exep)  # Compute the keyword counts for 'duplicated' records GND <- ParseProbDup(disGNdup2, Inf, F)$PRIM_ID  GNDKWCouts <- KWCounts(GN[GN$NationalID %in% GND, ],                        GNfields, exep)  # Compute the keyword counts for 'unique' records GNUKWCouts <- KWCounts(GN[!GN$NationalID %in% GND, ],                        GNfields, exep)  # Plot the counts as barplot par(mfrow = c(3,1))  bp1 <- barplot(table(GNKWCouts$COUNT),                xlab = \"Word count\", ylab = \"Frequency\",                main = \"A\", col = \"#1B9E77\") text(bp1, 0, table(GNKWCouts$COUNT),cex = 1, pos = 3) legend(\"topright\", paste(\"No. of records =\",                          nrow(GN)),        bty = \"n\")  bp2 <- barplot(table(GNDKWCouts$COUNT),                xlab = \"Word count\", ylab = \"Frequency\",                main = \"B\", col = \"#D95F02\") text(bp2, 0, table(GNDKWCouts$COUNT),cex = 1, pos = 3) legend(\"topright\", paste(\"No. of records =\",                    nrow(GN[GN$NationalID %in% GND, ])),        bty = \"n\")  bp3 <- barplot(table(GNUKWCouts$COUNT),                xlab = \"Word count\", ylab = \"Frequency\",                main = \"C\", col = \"#7570B3\") text(bp3, 0, table(GNUKWCouts$COUNT),cex = 1, pos = 3) legend(\"topright\", paste(\"No. of records =\",                    nrow(GN[!GN$NationalID %in% GND, ])),        bty = \"n\")"},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"citing-pgrdup","dir":"Articles","previous_headings":"","what":"Citing PGRdup","title":"An Introduction to `PGRdup` Package","text":"","code":"citation(\"PGRdup\") To cite the R package 'PGRdup' in publications use:    Aravind, J., Radhamani, J., Kalyani Srinivasan, Ananda Subhash, B.,   and Tyagi, R. K.  (2025).  PGRdup: Discover Probable Duplicates in   Plant Genetic Resources Collections. R package version 0.2.4.0.9000,   https://github.com/aravind-j/PGRdup,https://cran.r-project.org/package=PGRdup.  A BibTeX entry for LaTeX users is    @Manual{,     title = {PGRdup: Discover Probable Duplicates in Plant Genetic Resources Collections},     author = {J. Aravind and J. Radhamani and {Kalyani Srinivasan} and B. {Ananda Subhash} and Rishi Kumar Tyagi},     note = {R package version 0.2.4.0.9000 https://github.com/aravind-j/PGRdup, https://cran.r-project.org/package=PGRdup},     year = {2025},   }  This free and open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the package."},{"path":"https://aravind-j.github.io/PGRdup/articles/Introduction.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"An Introduction to `PGRdup` Package","text":"","code":"sessionInfo() R version 4.5.2 (2025-10-31) Platform: aarch64-apple-darwin20 Running under: macOS Sequoia 15.7.2  Matrix products: default BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib  LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1  locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8  time zone: UTC tzcode source: internal  attached base packages: [1] stats     graphics  grDevices utils     datasets  methods   base       other attached packages: [1] PGRdup_0.2.4.0.9000 gridExtra_2.3       wordcloud_2.6       [4] RColorBrewer_1.1-3  diagram_1.6.5       shape_1.4.6.1        loaded via a namespace (and not attached):  [1] sass_0.4.10          bitops_1.0-9         stringi_1.8.7         [4] digest_0.6.39        magrittr_2.0.4       evaluate_1.0.5        [7] grid_4.5.2           fastmap_1.2.0        jsonlite_2.0.0       [10] httr_1.4.7           scales_1.4.0         stringdist_0.9.15    [13] XML_3.99-0.20        microbenchmark_1.5.0 textshaping_1.0.4    [16] jquerylib_0.1.4      cli_3.6.5            rlang_1.1.6          [19] withr_3.0.2          cachem_1.1.0         yaml_2.3.12          [22] tools_4.5.2          parallel_4.5.2       ggplot2_4.0.1        [25] curl_7.0.0           vctrs_0.6.5          R6_2.6.1             [28] lifecycle_1.0.4      fs_1.6.6             ragg_1.5.0           [31] pkgconfig_2.0.3      desc_1.4.3           pkgdown_2.2.0        [34] bslib_0.9.0          pillar_1.11.1        gtable_0.3.6         [37] data.table_1.17.8    glue_1.8.0           Rcpp_1.1.0           [40] systemfonts_1.3.1    xfun_0.55            knitr_1.50           [43] farver_2.1.2         htmltools_0.5.9      igraph_2.2.1         [46] rmarkdown_2.30       labeling_0.4.3       compiler_4.5.2       [49] S7_0.2.1             RCurl_1.98-1.17"},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"J. Aravind. Author, maintainer. J. Radhamani. Author. Kalyani Srinivasan. Author. B. Ananda Subhash. Author. Rishi Kumar Tyagi. Author. ICAR-NBGPR. Copyright holder.           https://nbpgr.org./ Maurice Aubrey. Contributor.           Double Metaphone Kevin Atkinson. Contributor.           Double Metaphone Lawrence Philips. Contributor.           Double Metaphone","code":""},{"path":"https://aravind-j.github.io/PGRdup/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Aravind, J., Radhamani, J., Kalyani Srinivasan, Ananda Subhash, B., Tyagi, R. K.  ().  PGRdup: Discover Probable Duplicates Plant Genetic Resources Collections. R package version 0.2.4.0.9000, https://github.com/aravind-j/PGRdup,https://cran.r-project.org/package=PGRdup.","code":"@Manual{,   title = {PGRdup: Discover Probable Duplicates in Plant Genetic Resources Collections},   author = {J. Aravind and J. Radhamani and {Kalyani Srinivasan} and B. {Ananda Subhash} and Rishi Kumar Tyagi},   note = {R package version 0.2.4.0.9000 https://github.com/aravind-j/PGRdup, https://cran.r-project.org/package=PGRdup}, }"},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":null,"dir":"","previous_headings":"","what":"Version 0.2.3.9 - Second submission","title":"Version 0.2.3.9 - Second submission","text":"Restricted number threads used data.table OMP 2. Non standard files vignettes directory removed.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Microsoft Windows 11 Pro 10.0.22621, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release). local Ubuntu 20.04, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release). win-builder, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"ERRORs. WARNINGs except following. URL available probably false positive.","code":"Found the following (possibly) invalid URLs:     URL: https://dl.acm.org/doi/10.5555/349124.349132       From: man/DoubleMetaphone.Rd       Status: 403       Message: Forbidden"},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0239---second-submission-1","dir":"","previous_headings":"","what":"Version 0.2.3.9 - Second submission","title":"Version 0.2.3.9 - Second submission","text":"Restricted number threads used data.table OMP 2.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-1","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Microsoft Windows 11 Pro 10.0.22621, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release). local Ubuntu 20.04, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release). win-builder, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-1","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"ERRORs, WARNINGs.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0239---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.9 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Fixed issue reading CITATION file.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-2","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Microsoft Windows 11 Pro 10.0.22621, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release). local Ubuntu 20.04, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release). win-builder, R-release (R 4.3.1) & R-devel (R 4.4.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-2","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"ERRORs, WARNINGs.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0238---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.8 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Replace latin1 encoding UTF-8. Fixed non-ascii encoding.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-3","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Pro 21H2, R-release (R 4.3.0) & R-devel (R 4.4.0 Pre-release). local Ubuntu 20.04, R-release (R 4.3.0) & R-devel (R 4.4.0 Pre-release). win-builder, R-release (R 4.3.0) & R-devel (R 4.4.0 Pre-release). macOS builder, R-release (R 4.3.0).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-3","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"ERRORs, WARNINGs.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0237---third-submission","dir":"","previous_headings":"","what":"Version 0.2.3.7 - Third submission","title":"Version 0.2.3.9 - Second submission","text":"Fixed cairo_pdf unconditional use. Fixed xtfrm data.frame ValidatePrimKey(). Fixed issue vignette Solaris build.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-4","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1803, R-release (R 4.0.4) & R-devel (R 4.1.0 Pre-release). local Ubuntu 16.04, R-release (R 4.0.4) & R-devel (R 4.1.0 Pre-release). win-builder, R-release (R 4.0.4) & R-devel (R 4.1.0 Pre-release). rhub:solaris-x86-patched - i386-pc-solaris2.10 (32-bit), R-release (R 4.0.3). rhub:solaris-x86-patched-ods - i386-pc-solaris2.10 (32-bit), R-release (R 4.0.3). rhub:macos-highsierra-release-cran - x86_64-apple-darwin17.0 (64-bit), R-release (R 4.0.3).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0237---second-submission","dir":"","previous_headings":"","what":"Version 0.2.3.7 - Second submission","title":"Version 0.2.3.9 - Second submission","text":"Fixed cairo_pdf unconditional use. Fixed xtfrm data.frame ValidatePrimKey().","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-5","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1803, R-release (R 4.0.3) & R-devel (R 4.1.0 Pre-release). local Ubuntu 16.04, R-release (R 4.0.3) & R-devel (R 4.1.0 Pre-release). win-builder, R-release (R 4.0.3) & R-devel (R 4.1.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0237---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.7 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Fixed cairo_pdf unconditional use.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-6","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1803, R-release (R 4.0.3) & R-devel (R 4.1.0 Pre-release). local Ubuntu 16.04, R-release (R 4.0.3) & R-devel (R 4.1.0 Pre-release). win-builder, R-release (R 4.0.3) & R-devel (R 4.1.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0236---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.6 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Reverted using system certificates instead RCurl ones fetching displaying version history suggested Prof. Brian Ripley (ripley@stats.ox.ac.uk).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-7","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1803, R-release (R 4.0.2) & R-devel (R 4.1.0 Pre-release). local Ubuntu 16.04, R-release (R 4.0.2) & R-devel (R 4.1.0 Pre-release). win-builder, R-release (R 4.0.2) & R-devel (R 4.1.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-4","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"ERRORs WARNINGs. Regarding (possibly) invalid URLs found: URL: https://www.genesys-pgr.org/ : man/read.genesys.Rd Status: 502 Message: Bad Gateway seem false positives opening browser. Kindly advise.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0235---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.5 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Fixed problems missing link orphaned RecordLinkage package leading warning flags CRAN checks.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-8","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1809, R-release (R 3.6.2) & R-devel (R 3.7.0 Pre-release). local Ubuntu 16.04, R-release (R 3.6.2) & R-devel (R 3.7.0 Pre-release). win-builder, R-release (R 3.6.2) & R-devel (R 3.7.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0234---second-submission","dir":"","previous_headings":"","what":"Version 0.2.3.4 - Second submission","title":"Version 0.2.3.9 - Second submission","text":"Fixed issue missing vignette files ‘inst/doc’ leading failure CRAN pre-tests.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-9","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1809, R-release (R 3.6.1) & R-devel (R 3.7.0 Pre-release). local Ubuntu 16.04, R-release (R 3.6.1) & R-devel (R 3.7.0 Pre-release). win-builder, R-release (R 3.6.1) & R-devel (R 3.7.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0234---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.4 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Updated regular expressions PCRE2 compliant. Fixed issue underlying C code ‘strncpy’. Changed specified bound depending length source argument destination argument.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-10","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 10 Home v1809, R-release (R 3.6.1) & R-devel (R 3.7.0 Pre-release). local Ubuntu 16.04, R-release (R 3.6.1) & R-devel (R 3.7.0 Pre-release). win-builder, R-release (R 3.6.1) & R-devel (R 3.7.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0233---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.3 - First submission","title":"Version 0.2.3.9 - Second submission","text":"Use packages Suggests microbenchmark made conditional avoid problems available OS.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-11","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.4.3) & R-devel (R 3.5.0 Pre-release). local Ubuntu 16.04, R-release (R 3.4.3) & R-devel (R 3.5.0 Pre-release). win-builder, R-release (R 3.4.3) & R-devel (R 3.5.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0232---second-submission","dir":"","previous_headings":"","what":"Version 0.2.3.2 - Second submission","title":"Version 0.2.3.9 - Second submission","text":"Added copyright Authors@R along original contributors C code. memtest errors package corrected.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-12","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.4.1) & R-devel (R 3.5.0 Pre-release). local Ubuntu 16.04, R-release (R 3.4.1) & R-devel (R 3.5.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-0232---first-submission","dir":"","previous_headings":"","what":"Version 0.2.3.2 - First submission","title":"Version 0.2.3.9 - Second submission","text":"memtest errors package corrected.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-13","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.4.1) & R-devel (R 3.5.0 Pre-release). local Ubuntu 16.04, R-release (R 3.4.1) & R-devel (R 3.5.0 Pre-release). win-builder, R-release (R 3.4.1) & R-devel (R 3.5.0 Pre-release). CLANG memtest via rocker/r-devel-ubsan-clang. GCC memtest via rocker/r-devel-san.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-5","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"0 errors | 0 warnings | 0 notes","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-14","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.3.3) & R-devel (R 3.4.0 Pre-release). local Ubuntu 16.04, R-release (R 3.3.3) & R-devel (R 3.4.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-6","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"0 errors | 0 warnings | 0 notes","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-023---resubmission","dir":"","previous_headings":"","what":"Version 0.2.3 - Resubmission","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Converted non-confirming R-project URLs README.md files canonical form (https instead http).","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-15","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.3.3) & R-devel. local Ubuntu 12.04, R-release (R 3.3.3) & R-devel. win-builder, R-release (R 3.3.3) & R-devel (R 3.4.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-022---resubmission","dir":"","previous_headings":"","what":"Version 0.2.2 - Resubmission","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Converted non-confirming package page URLs README.md canonical form. Reduced size ‘Introduction.pdf’ ‘inst/doc’ using tools::compactPDF(gs_quality = \"ebook\").","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-16","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.2.3) & R-devel (R 3.3.0 Pre-release). local Ubuntu 12.04, R-release (R 3.2.3) & R-devel (R 3.3.0 Pre-release). win-builder, R-release (R 3.2.3) & R-devel (R 3.3.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"note","dir":"","previous_headings":"","what":"Note","title":"Version 0.2.3.9 - Second submission","text":"change email addresses four package authors including maintainer made. due implementation unified communications platform parent organization (ICAR) warranting close within institute (ICAR-NBPGR) mail server. confirmation sent previous address maintainer (aravindj@nbpgr.ernet.).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-021---resubmission-2","dir":"","previous_headings":"","what":"Version 0.2.1 - Resubmission 2","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Regarding (possibly) invalid URLs found: URL: http://www.nbpgr.ernet.:8080/PGRPortal/ : man/GN1000.Rd Status: Error Message: libcurl error code 7 Failed connect www.nbpgr.ernet.port 8080: Connection timed URLs temporarily active locally tests done. now running.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-021---resubmission","dir":"","previous_headings":"","what":"Version 0.2.1 - Resubmission","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Compacted vignette pdf. Fixed error rebuilding vignette outputs. Used following test environments local Windows 7 Ultimate install, R-release (R 3.1.3) & R-devel (R 3.3.0 Pre-release). local Ubuntu 12.04, R-release (R 3.1.3). win-builder R-release (R 3.1.3) & R-devel (R 3.3.0 Pre-release).","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-17","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R-release (R 3.1.3) & R-devel (R 3.3.0 Pre-release). local Ubuntu 12.04, R-release (R 3.1.3) & R-devel (R 3.3.0 Pre-release).","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-020---resubmission-4","dir":"","previous_headings":"","what":"Version 0.2.0 - Resubmission 4","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Fixed errors C code successful compilation installation package.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-020---resubmission-3","dir":"","previous_headings":"","what":"Version 0.2.0 - Resubmission 3","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Modified ‘Title’ field DESCRIPTION meaningful. Expanded PGR ‘Plant Genetic Resources’ Modified ‘Description’ field DESCRIPTION follows. Corrected mis-spelled words. Edited text limiting explaining specialized vocabulary far possible make intelligible CRAN users. ‘Genebank managers’ responsible curation collections living genetic resources seed tissue ‘Passport databases’ databases records identity collections. term dropped make description intelligible felt inessential context.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-020---resubmission-2","dir":"","previous_headings":"","what":"Version 0.2.0 - Resubmission 2","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Shortened ‘Title’ field DESCRIPTION 65 chars. Modified ‘Description’ field DESCRIPTION according manual.","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"version-020---resubmission","dir":"","previous_headings":"","what":"Version 0.2.0 - Resubmission","title":"Version 0.2.3.9 - Second submission","text":"resubmission. version : Used following test environments (Used R-devel according CRAN policy) local Windows 7 Ultimate (3.1.3 R-devel). local Ubuntu 12.04 (3.1.3 R-devel). win-builder (devel(3.2.0 alpha) release(3.1.3)). R CMD check –-cran results ERRORs WARNINGs. NOTES except “checking CRAN incoming feasibility”. Changed format author names appropriately. Regarding (possibly) invalid URLs: URL: http://www.nbpgr.ernet./ : README.md Status: Error Message: libcurl error code 6 resolve host: www.nbpgr.ernet.URL: http://www.nbpgr.ernet./PGRPortal : man/GN1000.Rd Status: Error Message: libcurl error code 6 resolve host: www.nbpgr.ernet.URLs temporarily active locally tests done. permanent redirection (HTTP/1.1 301) URL http://www.nbpgr.ernet./PGRPortal hence updated redirected URL (http://www.nbpgr.ernet.:8080/PGRPortal/) according CRAN policy. However please note temporary redirection (HTTP/1.1 302) http://www.nbpgr.ernet.:8080/PGRPortal/.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"test-environments-18","dir":"","previous_headings":"","what":"Test environments","title":"Version 0.2.3.9 - Second submission","text":"local Windows 7 Ultimate install, R 3.1.3 local Ubuntu 12.04, R 3.1.3 win-builder (devel release)","code":""},{"path":"https://aravind-j.github.io/PGRdup/cran-comments-archive.html","id":"r-cmd-check-results-7","dir":"","previous_headings":"","what":"R CMD check results","title":"Version 0.2.3.9 - Second submission","text":"ERRORs WARNINGs. NOTES except “checking CRAN incoming feasibility”.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"aravind-j1-radhamani-j1-kalyani-srinivasan1-ananda-subhash-b2-and-tyagi-r-k1","dir":"","previous_headings":"","what":"Aravind, J.1, Radhamani, J.1, Kalyani Srinivasan1, Ananda Subhash, B.2, and Tyagi, R. K.1","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"ICAR-National Bureau Plant Genetic Resources, New Delhi, India Centre Development Advanced Computing, Thiruvananthapuram, Kerala, India","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"introduction-","dir":"","previous_headings":"","what":"Introduction","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"R package PGRdup developed tool aid genebank managers identification probable duplicate accessions plant genetic resources (PGR) passport databases. package primarily implements workflow designed fetch groups sets germplasm accessions similar passport data particularly fields associated accession names within across PGR passport databases. functions package primarily built using following R packages: data.table igraph stringdist stringi","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"package can installed CRAN follows:","code":"# Install from CRAN install.packages('PGRdup', dependencies=TRUE)  The development version can be installed from github as follows: # Install development version from Github devtools::install_github(\"aravind-j/PGRdup\")"},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"workflow","dir":"","previous_headings":"","what":"Workflow","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"series steps involve workflow along associated functions illustrated :","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"step-1","dir":"","previous_headings":"Workflow","what":"Step 1","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"Function(s) : DataClean MergeKW MergePrefix MergeSuffix Use functions appropriate data standardisation relevant fields passport databases harmonize punctuation, leading zeros, prefixes, suffixes etc. associated accession names.","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"step-2","dir":"","previous_headings":"Workflow","what":"Step 2","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"Function(s) : KWIC Use function extract information relevant fields keywords text strings form searchable Keyword Context (KWIC) index.","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"step-3","dir":"","previous_headings":"Workflow","what":"Step 3","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"Function(s) : ProbDup Execute fuzzy, phonetic semantic matching keywords identify probable duplicate sets either within single KWIC index two indexes using function. fuzzy matching levenshtein edit distance used, phonetic matching, double metaphone algorithm used. semantic matching, synonym sets ‘synsets’ accession names can supplied input text strings sets treated identical matching. Various options tweak matching strategies used also available function.","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"step-4","dir":"","previous_headings":"Workflow","what":"Step 4","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"Function(s) : DisProbDup ReviewProbDup ReconstructProbDup Inspect, revise improve retrieved sets using functions. considerable intersections exist initially identified sets, DisProbDup may used get disjoint sets. identified sets may subjected clerical review transforming appropriate spreadsheet format contains raw data original database(s) using ReviewProbDup subsequently converted back using ReconstructProbDup.","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"adjuncts","dir":"","previous_headings":"Workflow","what":"Adjuncts","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"Function(s) : ValidatePrimKey DoubleMetaphone ParseProbDup AddProbDup SplitProbDup MergeProbDup ViewProbDup KWCounts read.genesys Use helper functions needed. ValidatePrimKey can used check whether column chosen data.frame primary primary key/ID confirms constraints absence duplicates NULL values. DoubleMetaphone implementation Double Metaphone phonetic algorithm R used phonetic matching. ParseProbDup AddProbDup work objects class ProbDup. former can used parse probable duplicate sets ProbDup object data.frame latter can used add sets data fields passport databases. SplitProbDup can used split object class ProbDup according set counts. MergeProbDup can used merge together two objects class ProbDup. ViewProbDup can used plot summary visualizations probable duplicate sets retrieved object class ProbDup. KWCounts can used compute keyword counts PGR passport database fields(columns), can give rough indication completeness data. read.genesys can used import PGR data Darwin Core - germplasm zip archive downloaded genesys database R environment.","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"tips","dir":"","previous_headings":"","what":"Tips","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"Use fread rapidly read large files instead read.csv read.table base. case PGR passport data DBMS, use appropriate R-database interface packages get required table data.frame R.","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"note","dir":"","previous_headings":"","what":"Note","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"ProbDup function can memory hungry large passport databases. cases, ensure system sufficient memory smooth functioning (See ?ProbDup).","code":""},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"detailed-tutorial","dir":"","previous_headings":"","what":"Detailed tutorial","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"detailed tutorial (vignette) used package type: vignette latest version also available online.","code":"browseVignettes(package = 'PGRdup')"},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"whats-new","dir":"","previous_headings":"","what":"What’s new","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"know whats new version type:","code":"news(package='PGRdup')"},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"links","dir":"","previous_headings":"","what":"Links","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"CRAN page Github page Documentation website Zenodo DOI","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/index.html","id":"citing-pgrdup","dir":"","previous_headings":"","what":"Citing PGRdup","title":"Discover Probable Duplicates in Plant Genetic Resources Collections","text":"cite methods package use:","code":"citation(\"PGRdup\") To cite the R package 'PGRdup' in publications use:    Aravind, J., Radhamani, J., Kalyani Srinivasan, Ananda Subhash, B.,   and Tyagi, R. K.  (2025).  PGRdup: Discover Probable Duplicates in   Plant Genetic Resources Collections. R package version 0.2.4.0.9000,   https://github.com/aravind-j/PGRdup,https://cran.r-project.org/package=PGRdup.  A BibTeX entry for LaTeX users is    @Manual{,     title = {PGRdup: Discover Probable Duplicates in Plant Genetic Resources Collections},     author = {J. Aravind and J. Radhamani and {Kalyani Srinivasan} and B. {Ananda Subhash} and Rishi Kumar Tyagi},     note = {R package version 0.2.4.0.9000 https://github.com/aravind-j/PGRdup, https://cran.r-project.org/package=PGRdup},     year = {2025},   }  This free and open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the package."},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"AddProbDup adds fuzzy, phonetic semantic probable duplicates sets data fields object class ProbDup original PGR passport database.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"","code":"AddProbDup(pdup, db, addto = c(\"I\", \"II\"), max.count = 30)"},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"pdup object class ProbDup. db data frame PGR passport database. addto Either \"\" \"II\" indicating database data.fields added (see Details). max.count maximum count probable duplicate sets whose information retrieved.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"data frame PGR passport database probable duplicate   sets fields added.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"function helps add information associated identified fuzzy, phonetic semantic probable duplicate sets using ProbDup function original PGR passport database. Associated data fields SET_NO, ID IDKW added based PRIM_ID field(column). case one KWIC index used generate object class ProbDup, argument addto can used specify database data fields added. default \"\" indicates database first KWIC index created \"II\" indicates database second index created.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"primary ID/key records fuzzy, phonetic semantic   duplicate sets found missing original database   db, ignored matching records   considered adding information warning. may due data standardization primary ID/key field using   function DataClean creation KWIC   index subsequent identification probable duplicate sets.   case, recommended use identical data standardization operation   database db running function.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/AddProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add probable duplicate sets fields to the PGR passport database — AddProbDup","text":"","code":"if (FALSE) { # \\dontrun{  #' # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")            # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                   semantic = TRUE, syn = syn)  # Add the duplicates sets to the original database                  GNwithdup <-  AddProbDup(pdup = GNdup, db = GN1000, addto = \"I\")                    } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/DataClean.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean PGR passport data — DataClean","title":"Clean PGR passport data — DataClean","text":"DataClean cleans data character vector according conditions arguments.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DataClean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean PGR passport data — DataClean","text":"","code":"DataClean(   x,   fix.comma = TRUE,   fix.semcol = TRUE,   fix.col = TRUE,   fix.bracket = TRUE,   fix.punct = TRUE,   fix.space = TRUE,   fix.sep = TRUE,   fix.leadzero = TRUE )"},{"path":"https://aravind-j.github.io/PGRdup/reference/DataClean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean PGR passport data — DataClean","text":"x character vector. , coerced character .character. fix.comma logical. TRUE, commas replaced space (see Details). fix.semcol logical. TRUE, semicolons replaced space (see Details). fix.col logical. TRUE, colons replaced space (see Details). fix.bracket logical. TRUE, brackets replaced space (see Details). fix.punct logical. TRUE, punctuation characters removed (see Details). fix.space logical. TRUE, space characters replaced space multiple spaces converted single space (see Details). fix.sep logical. TRUE, space alphabetic characters followed digits removed (see Details). fix.leadzero logical. TRUE, leading zeros removed (see Details).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DataClean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean PGR passport data — DataClean","text":"character vector cleaned data converted upper case.   NAs converted blank strings.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DataClean.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean PGR passport data — DataClean","text":"function aids standardization preparation PGR passport data creation KWIC index KWIC function identification probable duplicate accessions ProbDup function. cleans character strings passport data fields(columns) specified input character vector x according conditions arguments order. input vector x type character, coerced character vector. function designed particularly use fields corresponding accession names accession ids, collection numbers, accession names etc. essentially wrapper around gsub base function regex arguments. also converts strings upper case removes leading trailing spaces. Commas, semicolons colons sometimes used separate multiple strings names within field can replaced single space using logical arguments fix.comma, fix.semcol fix.col respectively. Similarly logical argument fix.bracket can used replace brackets including parenthesis, square brackets curly brackets space. logical argument fix.punct can used remove punctuation data. fix.space can used convert space characters tab, newline, vertical tab, form feed carriage return spaces finally convert multiple spaces single space. fix.sep can used merge together accession identifiers composed alphabetic characters separated series digits space character. example IR 64, PUSA 256 etc. fix.leadzero can used remove leading zeros accession name fields facilitate matching identify probable duplicates. e.g. IR0064 -> IR64","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/DataClean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean PGR passport data — DataClean","text":"","code":"names <- c(\"S7-12-6\", \"ICG-3505\", \"U 4-47-18;EC 21127\", \"AH 6481\", \"RS   1\",            \"AK 12-24\", \"2-5 (NRCG-4053)\", \"T78, Mwitunde\", \"ICG 3410\",            \"#648-4 (Gwalior)\", \"TG4;U/4/47/13\", \"EC0021003\") DataClean(names) #>  [1] \"S7126\"          \"ICG3505\"        \"U44718 EC21127\" \"AH6481\"         #>  [5] \"RS1\"            \"AK1224\"         \"25 NRCG4053\"    \"T78 MWITUNDE\"   #>  [9] \"ICG3410\"        \"6484 GWALIOR\"   \"TG4 U44713\"     \"EC21003\""},{"path":"https://aravind-j.github.io/PGRdup/reference/DisProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Get disjoint probable duplicate sets — DisProbDup","title":"Get disjoint probable duplicate sets — DisProbDup","text":"DisProbDup finds joins intersecting sets object class ProbDup get disjoint probable duplicate sets.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DisProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get disjoint probable duplicate sets — DisProbDup","text":"","code":"DisProbDup(pdup, combine = c(\"F\", \"P\", \"S\"))"},{"path":"https://aravind-j.github.io/PGRdup/reference/DisProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get disjoint probable duplicate sets — DisProbDup","text":"pdup object class ProbDup. combine character vector indicating type sets considered together retrieving disjoint sets. NULL, disjoint sets within type retrieved (see Details).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DisProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get disjoint probable duplicate sets — DisProbDup","text":"Returns object class ProbDup either disjoint   sets within type - FuzzyDuplicates, PhoneticDuplicates   SemanticDuplicates combine = NULL combined   disjoint duplicate sets additional element DisjointDupicates   according choice specified argument combine.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DisProbDup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get disjoint probable duplicate sets — DisProbDup","text":"function considers accession primary keys/IDs finding intersecting sets subsequently joins retrieve disjoint sets. operations implemented utilizing igraph package functions. Disjoint sets retrieved either individually type probable duplicate sets considering type sets simultaneously. case latter, disjoint type sets alone returned output additional data frame DisjointDuplicates object class ProbDup","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/DisProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get disjoint probable duplicate sets — DisProbDup","text":"","code":"if (FALSE) { # \\dontrun{  # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                  semantic = TRUE, syn = syn) lapply(GNdup, dim)  # Get disjoint probable duplicate sets of each kind disGNdup1 <- DisProbDup(GNdup, combine = NULL) lapply(disGNdup1, nrow)  # Get disjoint probable duplicate sets combining all the kinds of sets disGNdup2 <- DisProbDup(GNdup, combine = c(\"F\", \"P\", \"S\")) lapply(disGNdup2, nrow)  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":null,"dir":"Reference","previous_headings":"","what":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"DoubleMetaphone converts strings double metaphone phonetic codes.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"","code":"DoubleMetaphone(str)"},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"str character vector whose strings encoded double metaphone algorithm.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"Returns list two character vectors length   input vector. first character vector contains primary double   metaphone encodings, second character vector contains   alternate encodings.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"implementation Double Metaphone phonetic algorithm R. non-ASCII characters encountered input character vector str, warning issued transliterated accented characters converted ASCII unaccented versions.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"case non-ASCII characters strings, warning issued   accented characters converted ASCII unaccented versions.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"acknowledgement","dir":"Reference","previous_headings":"","what":"Acknowledgement","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"C code double metaphone   algorithm adapted Maurice Aubrey's perl module hosted   gitpan/Text-DoubleMetaphone   public    github library along corresponding   license    information.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"Philips, Lawrence. 2000. \"Double Metaphone Search Algorithm.\"   C/C++ Users Journal 18 (6): 38-43.   https://dl.acm.org/doi/10.5555/349124.349132.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/DoubleMetaphone.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"'Double Metaphone' phonetic algorithm — DoubleMetaphone","text":"","code":"# Return the primary and secondary Double Metaphone encodings for a character vector. str1 <- c(\"Jyothi\", \"Jyoti\") str2 <- c(\"POLLACHI\", \"BOLLACHI\") DoubleMetaphone(str1) #> $primary #> [1] \"J0\" \"JT\" #>  #> $alternate #> [1] \"AT\" \"AT\" #>  DoubleMetaphone(str2) #> $primary #> [1] \"PLX\" \"PLX\" #>  #> $alternate #> [1] \"PLK\" \"PLK\" #>  if (FALSE) { # \\dontrun{ # Issue a warning in case of non-ASCII characters. str3 <- c(\"J\\xf5geva\", \"Jogeva\") DoubleMetaphone(str3) } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/GN1000.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample groundnut PGR passport data — GN1000","title":"Sample groundnut PGR passport data — GN1000","text":"Sample PGR passport data 1000 groundnut accessions held Indian National Genebank National Bureau Plant Genetic Resources (NBPGR), New Delhi.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/GN1000.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample groundnut PGR passport data — GN1000","text":"","code":"data(GN1000)"},{"path":"https://aravind-j.github.io/PGRdup/reference/GN1000.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample groundnut PGR passport data — GN1000","text":"data frame 1000 records following 10   columns(fields): CommonName Common name BotanicalName Botanical name NationalID NBPGR National   identifier CollNo Collector number DonorID Donor ID OtherID1 ID field 1 OtherID2 ID field 2 BioStatus Biological status SourceCountry Country   origin TransferYear Year transfer","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate keyword counts — KWCounts","title":"Generate keyword counts — KWCounts","text":"KWCounts generates keyword counts PGR passport database fields(columns).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate keyword counts — KWCounts","text":"","code":"KWCounts(x, fields, excep)"},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate keyword counts — KWCounts","text":"x data frame. fields character vector names fields(columns) data frame KWIC index generated. first field considered primary key identifier (see Details). excep vector keywords considered counts (see Details).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate keyword counts — KWCounts","text":"data frame keyword counts record.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate keyword counts — KWCounts","text":"function computes keyword counts PGR passport database fields(columns) specified fields argument. first field considered primary key identifier used counting keywords. strings given excep argument ignored generating counts. keyword counts can give rough indication completeness data database fields used identification probable duplicates.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Generate keyword counts — KWCounts","text":"large number exceptions /large data.frame computation keyword counts may take time.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/KWCounts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate keyword counts — KWCounts","text":"","code":"# Load PGR passport database GN <- GN1000  # Specify database fields to use as a vector GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")           # Compute the keyword counts GNKWCouts <- KWCounts(GN, GNfields, exep)  # Plot the keyword counts bp <- barplot(table(GNKWCouts$COUNT),               xlab = \"Word count\", ylab = \"Frequency\", col = \"#CD5555\") text(bp, 0, table(GNKWCouts$COUNT),cex=1,pos=3)"},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a KWIC index — KWIC","title":"Create a KWIC index — KWIC","text":"KWIC creates Keyword Context index PGR passport database fields.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a KWIC index — KWIC","text":"","code":"KWIC(x, fields, min.freq = 10)"},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a KWIC index — KWIC","text":"x data frame KWIC index generated. fields character vector names fields(columns) data frame KWIC index generated. first field considered primary key identifier (see Details). min.freq Frequency keywords computed min.freq. Default 10.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a KWIC index — KWIC","text":"list class KWIC containing following components:","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a KWIC index — KWIC","text":"function generates Keyword Context index data frame PGR passport database based fields(columns) stated arguments, using data.table package. first element vector fields considered primary key identifier uniquely identifies rows data frame. Cleaning data input fields(columns) using DataClean function appropriate arguments suggested running function.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Create a KWIC index — KWIC","text":"Knüpffer, H. 1988. \"European Barley Database ECP/GR:   Introduction.\" Die Kulturpflanze 36 (1): 135-62.   doi:10.1007/BF01986957 . Knüpffer, H., L. Frese, M. W. M. Jongen. 1997. \"Using Central Crop   Databases: Searching Duplicates Gaps.\" Central Crop   Databases: Tools Plant Genetic Resources Management. Report   Workshop, Budapest, Hungary, 13-16 October 1996, edited E. Lipman, M.   W. M. Jongen, T. J. L. van Hintum, T. Gass, L. Maggioni, 67-77. Rome,   Italy Wageningen, Netherlands: International Plant Genetic   Resources Institute Centre Genetic Resources.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/KWIC.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a KWIC index — KWIC","text":"","code":"# Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x))  if (FALSE) { # \\dontrun{  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields) GNKWIC  # Retrieve the KWIC index from the KWIC object KWIC <- GNKWIC[[1]]  # Retrieve the keyword frequencies from the KWIC object KeywordFreq <- GNKWIC[[2]]  # Show error in case of duplicates and NULL values # in the primary key/ID field \"NationalID\" GN[1001:1005,] <- GN[1:5,] GN[1001,3] <- \"\" GNKWIC <- KWIC(GN, GNfields) } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeKW.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge keyword strings — MergeKW","title":"Merge keyword strings — MergeKW","text":"functions merge keyword strings separated delimiters space, period dash character vector single keyword strings.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeKW.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge keyword strings — MergeKW","text":"","code":"MergeKW(x, y, delim = c(\"space\", \"dash\", \"period\"))  MergePrefix(x, y, delim = c(\"space\", \"dash\", \"period\"))  MergeSuffix(x, y, delim = c(\"space\", \"dash\", \"period\"))"},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeKW.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge keyword strings — MergeKW","text":"x character vector. , coerced character .character. y list character vectors pairs strings merged (MergeKW) character vector strings merged succeeding string (MergePrefix) preceding string (MergeSuffix). type character, coerced .character. delim Delimiting characters removed keywords.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeKW.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge keyword strings — MergeKW","text":"character vector length x required   keyword strings merged.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeKW.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge keyword strings — MergeKW","text":"functions aid standardization relevant data fields(columns) PGR passport data creation KWIC index KWIC function subsequent identification probable duplicate accessions ProbDup function. recommended run function using DataClean function relevant data fields(columns) PGR passport databases. MergeKW merges together pairs strings specified list argument y wherever exist character vector. second string pair merged even followed number. MergePrefix merges prefix strings specified character vector argument y succeeding root word, wherever exist character vector. MergeSuffix merges suffix strings specified character vector argument y preceding root word, wherever exist character vector. suffix strings followed numbers also merged.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeKW.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge keyword strings — MergeKW","text":"","code":"names <- c(\"Punjab Bold\", \"Gujarat- Dwarf\", \"Nagpur.local\", \"SAM COL 144\",            \"SAM COL--280\", \"NIZAMABAD-LOCAL\", \"Dark Green Mutant\",            \"Dixie-Giant\", \"Georgia- Bunch\", \"Uganda-erect\", \"Small Japan\",            \"Castle  Cary\", \"Punjab erect\", \"Improved small japan\",            \"Dark Purple\")  # Merge pairs of strings y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"),            c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"),            c(\"Mota\", \"Company\")) names <- MergeKW(names, y1, delim = c(\"space\", \"dash\", \"period\"))  # Merge prefix strings y2 <- c(\"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") names <- MergePrefix(names, y2, delim = c(\"space\", \"dash\", \"period\"))  # Merge suffix strings y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") names <- MergeSuffix(names, y3, delim = c(\"space\", \"dash\", \"period\"))"},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge two objects of class ProbDup — MergeProbDup","title":"Merge two objects of class ProbDup — MergeProbDup","text":"MergeProbDup merges two objects class ProbDup single one.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge two objects of class ProbDup — MergeProbDup","text":"","code":"MergeProbDup(pdup1, pdup2)"},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge two objects of class ProbDup — MergeProbDup","text":"pdup1 object class ProbDup. pdup2 object class ProbDup.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge two objects of class ProbDup — MergeProbDup","text":"object class ProbDup merged list  fuzzy, phonetic   semantic probable duplicate sets.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/MergeProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge two objects of class ProbDup — MergeProbDup","text":"","code":"if (FALSE) { # \\dontrun{ #' # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")            # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                   semantic = TRUE, syn = syn)                   # Split the probable duplicate sets GNdupSplit <- SplitProbDup(GNdup, splitat = c(10, 10, 10))  # Merge the split sets GNdupMerged <- MergeProbDup(GNdupSplit[[1]], GNdupSplit[[3]])  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/PGRdup-package.html","id":null,"dir":"Reference","previous_headings":"","what":"The PGRdup Package — PGRdup-package","title":"The PGRdup Package — PGRdup-package","text":"Functions facilitate genebank managers identification probable duplicate accessions plant genetic resources (PGR) passport databases.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/PGRdup-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"The PGRdup Package — PGRdup-package","text":"J Aravind j.aravind@icar.org. J Radhamani   jalli.radhamani@icar.gov. Kalyani Srinivasan   kalyani.srinivasan@icar.gov. B Ananda Subhash   anandasubhash@gmail.com  RK Tyagi   rishi.tyagi@icar.gov.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ParseProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse an object of class ProbDup to a data frame. — ParseProbDup","title":"Parse an object of class ProbDup to a data frame. — ParseProbDup","text":"ParseProbDup converts object class ProbDup data frame export.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ParseProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse an object of class ProbDup to a data frame. — ParseProbDup","text":"","code":"ParseProbDup(pdup, max.count = 30, insert.blanks = TRUE)"},{"path":"https://aravind-j.github.io/PGRdup/reference/ParseProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse an object of class ProbDup to a data frame. — ParseProbDup","text":"pdup object class ProbDup. max.count maximum count probable duplicate sets parsed data frame. insert.blanks logical. TRUE, inserts row NAs set.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ParseProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse an object of class ProbDup to a data frame. — ParseProbDup","text":"data frame long/narrow form probable duplicate sets   data following core columns:   retrieved columns(fields) prefix K* indicates KWIC index   origin.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/ParseProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse an object of class ProbDup to a data frame. — ParseProbDup","text":"","code":"if (FALSE) { # \\dontrun{  #' # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")            # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                   semantic = TRUE, syn = syn)                   # Convert to data frame of sets                GNdupParsed <- ParseProbDup(GNdup)  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify probable duplicates of accessions — ProbDup","title":"Identify probable duplicates of accessions — ProbDup","text":"ProbDup identifies probable duplicates germplasm accessions KWIC indexes created PGR passport databases using fuzzy, phonetic semantic matching strategies.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify probable duplicates of accessions — ProbDup","text":"","code":"ProbDup(   kwic1,   kwic2 = NULL,   method = c(\"a\", \"b\", \"c\"),   excep = NULL,   chunksize = 1000,   useBytes = TRUE,   fuzzy = TRUE,   max.dist = 3,   force.exact = TRUE,   max.alpha = 4,   max.digit = Inf,   phonetic = TRUE,   encoding = c(\"primary\", \"alternate\"),   phon.min.alpha = 5,   min.enc = 3,   semantic = FALSE,   syn = NULL )"},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify probable duplicates of accessions — ProbDup","text":"kwic1 object class KWIC. kwic2 object class KWIC. Required method \"b\" \"c\" (see Details). method method followed identification probable duplicates. Either \"\", \"b\" \"c\". (see Details). excep vector keywords KWIC used probable duplicate search (see Details). chunksize value indicating size KWIC index keyword block used searching matches time case large number keywords(see Note). useBytes logical. TRUE, performs byte-wise comparison instead character-wise comparison (see Note). fuzzy logical. TRUE identifies probable duplicates based fuzzy matching. max.dist maximum levenshtein distance keyword strings allowed match. Default 3 (see Details). force.exact logical. TRUE, enforces exact matching instead fuzzy matching keyword strings match criteria specified arguments max.alpha max.digit (see Details). max.alpha Maximum number alphabet characters present keyword string exact matching enforced rather fuzzy matching. Default 4 (see Details). max.digit Maximum number numeric characters present keyword string exact matching enforced rather fuzzy matching. Default Inf (see Details). phonetic logical. TRUE identifies probable duplicates based phonetic matching. encoding Double metaphone encoding phonetic matching. default \"primary\" (see Details). phon.min.alpha Minimum number alphabet characters present keyword string phonetic matching (see Details). min.enc Minimum number characters present double metaphone encoding keyword string phonetic matching (see Details). semantic logical. TRUE identifies probable duplicates based semantic matching. syn list character vectors synsets (see Details).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify probable duplicates of accessions — ProbDup","text":"list class ProbDup containing following data frames   probable duplicate sets identified along corresponding keywords   set counts: FuzzyDuplicates PhoneticDuplicates SemanticDuplicates data frame   following columns: prefix [K*] indicates KWIC index origin KEYWORD   PRIM_ID.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Identify probable duplicates of accessions — ProbDup","text":"function performs fuzzy, phonetic semantic matching keywords KWIC indexes PGR passport databases (created using KWIC function) identify probable duplicates germplasm accessions. function can execute matching according either following three methods specified method argument. Method : Perform string matching keywords single KWIC index identify probable duplicates accessions single PGR passport database. Method b: Perform string matching keywords first KWIC index (query) keywords second index (source) identify probable duplicates accessions first PGR passport database among accessions second database. Method c: Perform string matching keywords two different KWIC indexes jointly identify probable duplicates accessions among two PGR passport databases. Fuzzy matching approximate string matching keywords carried computing generalized levenshtein (edit) distance . distance measure  counts number deletions, insertions substitutions necessary turn one string another. distance max.dist considered match. Exact matching enforced argument force.exact TRUE. can used avoid fuzzy matching number alphabet characters keywords lesser critical value (max.alpha). Similarly, value max.digit can also set according requirements. default value Inf avoids fuzzy matching enforces exact matching keywords numerical characters. max.digit max.alpha set Inf, exact matching enforced keywords. exact matching enforced, keywords alphabet numeric characters number alphabet characters greater max.digit, matching carried separately alphabet numeric characters present. Phonetic matching keywords carried using Double Metaphone phonetic algorithm (DoubleMetaphone) identify keywords similar pronunciation. Either primary alternate encodings can used specifying encoding argument. argument phon.min.alpha sets limits number alphabet characters present string executing phonetic matching. Similarly min.enc sets limits number characters present encoding keyword phonetic matching. Semantic matching matches keywords based list accession name synonyms supplied list character vectors synonym sets (synsets) syn argument. Synonyms context refers interchangeable identifiers names accession recognized. Multiple keywords specified members synset syn merged together. facilitate accurate identification synonyms KWIC index, identical data standardization operations using MergeKW DataClean functions original database fields synset list recommended. probable duplicate sets identified initially may intersecting sets. get disjoint sets union intersecting sets use DisProbDup function. function AddProbDup can used add information associated identified sets object class ProbDup fields(columns) original PGR passport database. string matching operations executed stringdist-package functions.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Identify probable duplicates of accessions — ProbDup","text":"number keywords KWIC indexes increases, memory   consumption function also increases. string matching,   function relies upon creation \\(n\\)*\\(m\\) matrix possible   keyword pairs comparison, \\(n\\) \\(m\\) number   keywords query source indexes respectively. can lead   allocate vector size errors case large KWIC   indexes comparison matrix large reside memory.   case, try adjust chunksize argument get   appropriate size KWIC index keyword block used searching   matches time. However smaller chunksize may lead longer   computation time due memory-time trade-. progress matching displayed console number blocks   completed total (e.g. 6 / 30), percentage achievement (e.g.   30%) text-based progress bar. case multi-byte characters keywords, matching speed   dependent upon useBytes argument described   Encoding issues stringdist   function, made use string matching.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Identify probable duplicates of accessions — ProbDup","text":"van der Loo, M. P. J. 2014. \"Stringdist Package   Approximate String Matching.\" R Journal 6 (1):111-22.   https://journal.r-project.org/articles/RJ-2014-011/index.html.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/ProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identify probable duplicates of accessions — ProbDup","text":"","code":"if (FALSE) { # \\dontrun{  # Method \"a\" #===========  # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                  semantic = TRUE, syn = syn) GNdup  # Method \"b and c\" #=================  # Load PGR passport databases GN1 <- GN1000[!grepl(\"^ICG\", GN1000$DonorID), ] GN1$DonorID <- NULL GN2 <- GN1000[grepl(\"^ICG\", GN1000$DonorID), ] GN2 <- GN2[!grepl(\"S\", GN2$DonorID), ] GN2$NationalID <- NULL  # Specify as a vector the database fields to be used GN1fields <- c(\"NationalID\", \"CollNo\", \"OtherID1\", \"OtherID2\") GN2fields <- c(\"DonorID\", \"CollNo\", \"OtherID1\", \"OtherID2\")  # Clean the data GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) DataClean(x)) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Remove duplicated DonorID records in GN2 GN2 <- GN2[!duplicated(GN2$DonorID), ]  # Generate KWIC index GN1KWIC <- KWIC(GN1, GN1fields) GN2KWIC <- KWIC(GN2, GN2fields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdupb <- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = \"b\",                   excep = exep, fuzzy = TRUE, phonetic = TRUE,                   encoding = \"primary\", semantic = TRUE, syn = syn) GNdupb  GNdupc <- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = \"c\",                   excep = exep, fuzzy = TRUE, phonetic = TRUE,                   encoding = \"primary\", semantic = TRUE, syn = syn) GNdupc  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/ReconstructProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Reconstruct an object of class ProbDup — ReconstructProbDup","title":"Reconstruct an object of class ProbDup — ReconstructProbDup","text":"ReconstructProbDup reconstructs data frame probable duplicate sets created using function ReviewProbDup subjected manual clerical review, back object class ProbDup.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReconstructProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reconstruct an object of class ProbDup — ReconstructProbDup","text":"","code":"ReconstructProbDup(rev)"},{"path":"https://aravind-j.github.io/PGRdup/reference/ReconstructProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reconstruct an object of class ProbDup — ReconstructProbDup","text":"rev data frame core columns(fields) SET_NO, TYPE, K, PRIM_ID, DEL, SPLIT, COUNT IDKW","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReconstructProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reconstruct an object of class ProbDup — ReconstructProbDup","text":"object class ProbDup modified fuzzy,   phonetic semantic probable duplicate sets according instructions   specified clerical review.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReconstructProbDup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Reconstruct an object of class ProbDup — ReconstructProbDup","text":"data frame created using function ReviewProbDup object class ProbDup manual clerical review identified probable duplicate sets can reconstituted back object review using function. instructions modifying sets entered appropriate format columns DEL SPLIT clerical review taken account reconstituting probable duplicate sets. records Y column DEL deleted records identical integers column SPLIT default 0 reassembled new set.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/ReconstructProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reconstruct an object of class ProbDup — ReconstructProbDup","text":"","code":"if (FALSE) { # \\dontrun{  # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")            # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                   semantic = TRUE, syn = syn)  # Get disjoint probable duplicate sets of each kind disGNdup <- DisProbDup(GNdup, combine = NULL)  # Get the data frame for reviewing the duplicate sets identified RevGNdup <- ReviewProbDup(pdup = disGNdup, db1 = GN1000,                           extra.db1 = c(\"SourceCountry\", \"TransferYear\"),                            max.count = 30, insert.blanks = TRUE) # Examine and review the duplicate sets using edit function RevGNdup <- edit(RevGNdup)  # Examine and make changes to a set subset(RevGNdup, SET_NO==12 & TYPE==\"P\", select= c(IDKW, DEL, SPLIT)) RevGNdup[c(110, 112, 114, 118, 121, 122, 124), 6] <- \"Y\" RevGNdup[c(111, 115, 128), 7] <- 1 RevGNdup[c(113, 117, 120), 7] <- 2 RevGNdup[c(116, 119), 7] <- 3 RevGNdup[c(123, 125), 7] <- 4 RevGNdup[c(126, 127), 7] <- 5 subset(RevGNdup, SET_NO==12 & TYPE==\"P\", select= c(IDKW, DEL, SPLIT))  # Reconstruct ProDup object GNdup2 <- ReconstructProbDup(RevGNdup) lapply(disGNdup, nrow) lapply(GNdup2, nrow)  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"ReviewProbDup retrieves information associated probable duplicate sets original PGR passport database(s) identified order facilitate manual clerical review.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"","code":"ReviewProbDup(   pdup,   db1,   db2 = NULL,   extra.db1 = NULL,   extra.db2 = NULL,   max.count = 30,   insert.blanks = TRUE )"},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"pdup object class ProbDup. db1 data frame PGR passport database. db2 data frame PGR passport database. Required pdup created using one KWIC Index. extra.db1 character vector extra db1 column names retrieved. extra.db2 character vector extra db2 column names retrieved. max.count maximum count probable duplicate sets whose information retrieved. insert.blanks logical. TRUE, inserts row NAs set.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"data frame long/narrow form probable duplicate sets   data along associated fields original database(s). core   columns resulting data frame follows:   retrieved columns(fields) prefix K* indicates KWIC index   origin.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"function helps retrieve PGR passport information associated fuzzy, phonetic semantic probable duplicate sets object class ProbDup original databases(s) identified. original information accessions comprising set, subjected data standardization can compared manual clerical review validation set. default fields(columns) used initially creation KWIC indexes using KWIC function retrieved. Additional fields(columns) necessary can specified using extra.db1 extra.db2 arguments. output data frame can subjected clerical review either exporting external spreadsheet using write.csv function using edit function. column DEL can used indicate whether record deleted set . Y indicates \"Yes\", default N indicates \"\". column SPLIT similarly can used indicate whether record set branched new set. set identical integers column default 0 can used indicate removed assembled new set.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"primary ID/key records fuzzy, phonetic semantic   duplicate sets found missing original databases   db1 db2, ignored matching   records considered retrieving information warning. may due data standardization primary ID/key field using   function DataClean creation KWIC   index subsequent identification probable duplicate sets.   case, recommended use identical data standardization operation   databases db1 db2 running function. R <= v3.0.2, due copying named objects list(),   Invalid .internal.selfref detected fixed... warning can appear,   may safely ignored.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/ReviewProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve probable duplicate set information from PGR passport database for review — ReviewProbDup","text":"","code":"if (FALSE) { # \\dontrun{  # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",          \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",          \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",          \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",          \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",          \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                  semantic = TRUE, syn = syn)  # Get disjoint probable duplicate sets of each kind disGNdup <- DisProbDup(GNdup, combine = NULL)  # Get the data frame for reviewing the duplicate sets identified RevGNdup <- ReviewProbDup(pdup = disGNdup, db1 = GN1000,                           extra.db1 = c(\"SourceCountry\", \"TransferYear\"),                           max.count = 30, insert.blanks = TRUE) # Examine and review the duplicate sets using edit function RevGNdup <- edit(RevGNdup)  # OR examine and review the duplicate sets after exporting them as a csv file write.csv(file=\"Duplicate sets for review.csv\", x=RevGNdup)  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/SplitProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Split an object of class ProbDup — SplitProbDup","title":"Split an object of class ProbDup — SplitProbDup","text":"SplitProbDup splits object class ProbDup two basis set counts.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/SplitProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split an object of class ProbDup — SplitProbDup","text":"","code":"SplitProbDup(pdup, splitat = c(30, 30, 30))"},{"path":"https://aravind-j.github.io/PGRdup/reference/SplitProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split an object of class ProbDup — SplitProbDup","text":"pdup object class ProbDup. splitat vector 3 integers indicating set count Fuzzy, Phonetic Semantic duplicate sets pdup split.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/SplitProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split an object of class ProbDup — SplitProbDup","text":"list divided objects class ProbDup   (pdup1 pdup2) along corresponding lists   accessions present (list1 list2).","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/SplitProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split an object of class ProbDup — SplitProbDup","text":"","code":"if (FALSE) { # \\dontrun{ # Load PGR passport database GN <- GN1000  # Specify as a vector the database fields to be used GNfields <- c(\"NationalID\", \"CollNo\", \"DonorID\", \"OtherID1\", \"OtherID2\")  # Clean the data GN[GNfields] <- lapply(GN[GNfields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"), c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"), c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN[GNfields] <- lapply(GN[GNfields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN[GNfields] <- lapply(GN[GNfields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Generate KWIC index GNKWIC <- KWIC(GN, GNfields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")            # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  # Fetch probable duplicate sets GNdup <- ProbDup(kwic1 = GNKWIC, method = \"a\", excep = exep, fuzzy = TRUE,                  phonetic = TRUE, encoding = \"primary\",                   semantic = TRUE, syn = syn)                   # Split the probable duplicate sets GNdupSplit <- SplitProbDup(GNdup, splitat = c(10, 10, 10))  } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/ValidatePrimKey.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","title":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","text":"ValidatePrimKey checks column data frame confirms primary key/ID constraints absence duplicates NULL values. Aberrant records encountered returned output list.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ValidatePrimKey.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","text":"","code":"ValidatePrimKey(x, prim.key)"},{"path":"https://aravind-j.github.io/PGRdup/reference/ValidatePrimKey.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","text":"x data frame. prim.key character vector indicating name data frame column validated primary key/ID constraints (see Details).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ValidatePrimKey.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","text":"list containing following components:","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ValidatePrimKey.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","text":"function checks whether field(column) data frame PGR passport database confirms primary key/ID constraints absence duplicates NULL values. records nonconforming values column encountered, returned output list rectification. multiple fields(columns) given character vector prim.key field, first element considered primary key/ID field(column). Cleaning data input field(column) using DataClean function appropriate arguments suggested running function. recommended run function rectify aberrant records PGR passport database creating KWIC index using KWIC function.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/ValidatePrimKey.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate if a data frame column confirms to primary key/ID constraints — ValidatePrimKey","text":"","code":"GN <- GN1000 ValidatePrimKey(x=GN, prim.key=\"NationalID\") #> OK: No duplicated records found in prim.key field #> OK: No NULL records found in prim.key field #> $message1 #> [1] \"OK: No duplicated records found in prim.key field\" #>  #> $Duplicates #> NULL #>  #> $message2 #> [1] \"OK: No NULL records found in prim.key field\" #>  #> $NullRecords #> NULL #>  if (FALSE) { # \\dontrun{ # Show error in case of duplicates and NULL values  # in the primary key/ID field \"NationalID\" GN[1001:1005,] <- GN[1:5,] GN[1001,3] <- \"\" ValidatePrimKey(x=GN, prim.key=\"NationalID\")} # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/ViewProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","title":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","text":"ViewProbDup plots summary visualizations accessions within probable duplicate sets retrieved ProbDup object according grouping factor field(column) original database(s).","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ViewProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","text":"","code":"ViewProbDup(   pdup,   db1,   db2 = NULL,   factor.db1,   factor.db2 = NULL,   max.count = 30,   select,   order = \"type\",   main = NULL )"},{"path":"https://aravind-j.github.io/PGRdup/reference/ViewProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","text":"pdup object class ProbDup. db1 data frame PGR passport database. db2 data frame PGR passport database. Required pdup created using one KWIC Index. factor.db1 db1 column considered grouping accessions. class character factor. factor.db2 db2 column considered grouping accessions. class character factor. retrieved. max.count maximum count probable duplicate sets whose information plotted (see Note). select character vector factor names factor.db1 /factor.db2 considered grouping accessions (see Note). order order type sets retrieved plot. default \"type\" (see Details). main title plot.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ViewProbDup.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","text":"list containing following objects:","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/ViewProbDup.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","text":"primary ID/key records fuzzy, phonetic semantic   duplicate sets found missing original databases   db1 db2, ignored matching   records considered visualization. may due data standardization primary ID/key field using   function DataClean creation KWIC   index subsequent identification probable duplicate sets.   case, recommended use identical data standardization operation   databases db1 db2 running function.   summary visualization set information object class   ProbDup ViewProbDup, disjoint retrieved sets   made use , meaningful raw sets retrieved.   recommended disjoint sets obtained using   DisProbDup used input pdup. accession records sets count > max.count   considered unique. factor levels factor.db1 /factor.db2 columns   corresponding mentioned select argument alone   considered visualization. factor levels grouped   together single level named \"Others\". argument order can used specify order   type sets retrieved plotted visualization. default   \"type\" order according kind sets, \"sets\"   order according number sets kind \"acc\"   order according number accessions kind. individual plots made using ggplot   grouped together using gridExtra-package.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/ViewProbDup.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize the probable duplicate sets retrieved in a ProbDup object — ViewProbDup","text":"","code":"if (FALSE) { # \\dontrun{  # Method \"b and c\" #=================  # Load PGR passport databases GN1 <- GN1000[!grepl(\"^ICG\", GN1000$DonorID), ] GN1$DonorID <- NULL GN2 <- GN1000[grepl(\"^ICG\", GN1000$DonorID), ] GN2 <- GN2[!grepl(\"S\", GN2$DonorID), ] GN2$NationalID <- NULL  GN1$SourceCountry <- toupper(GN1$SourceCountry) GN2$SourceCountry <- toupper(GN2$SourceCountry)  GN1$SourceCountry <- gsub(\"UNITED STATES OF AMERICA\", \"USA\", GN1$SourceCountry) GN2$SourceCountry <- gsub(\"UNITED STATES OF AMERICA\", \"USA\", GN2$SourceCountry)  # Specify as a vector the database fields to be used GN1fields <- c(\"NationalID\", \"CollNo\", \"OtherID1\", \"OtherID2\") GN2fields <- c(\"DonorID\", \"CollNo\", \"OtherID1\", \"OtherID2\")  # Clean the data GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) DataClean(x)) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) DataClean(x)) y1 <- list(c(\"Gujarat\", \"Dwarf\"), c(\"Castle\", \"Cary\"), c(\"Small\", \"Japan\"),            c(\"Big\", \"Japan\"), c(\"Mani\", \"Blanco\"), c(\"Uganda\", \"Erect\"),            c(\"Mota\", \"Company\")) y2 <- c(\"Dark\", \"Light\", \"Small\", \"Improved\", \"Punjab\", \"SAM\") y3 <- c(\"Local\", \"Bold\", \"Cary\", \"Mutant\", \"Runner\", \"Giant\", \"No.\",         \"Bunch\", \"Peanut\") GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN1[GN1fields] <- lapply(GN1[GN1fields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) MergeKW(x, y1, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) MergePrefix(x, y2, delim = c(\"space\", \"dash\"))) GN2[GN2fields] <- lapply(GN2[GN2fields], function(x) MergeSuffix(x, y3, delim = c(\"space\", \"dash\")))  # Remove duplicated DonorID records in GN2 GN2 <- GN2[!duplicated(GN2$DonorID), ]  # Generate KWIC index GN1KWIC <- KWIC(GN1, GN1fields) GN2KWIC <- KWIC(GN2, GN2fields)  # Specify the exceptions as a vector exep <- c(\"A\", \"B\", \"BIG\", \"BOLD\", \"BUNCH\", \"C\", \"COMPANY\", \"CULTURE\",           \"DARK\", \"E\", \"EARLY\", \"EC\", \"ERECT\", \"EXOTIC\", \"FLESH\", \"GROUNDNUT\",           \"GUTHUKAI\", \"IMPROVED\", \"K\", \"KUTHUKADAL\", \"KUTHUKAI\", \"LARGE\",           \"LIGHT\", \"LOCAL\", \"OF\", \"OVERO\", \"P\", \"PEANUT\", \"PURPLE\", \"R\",           \"RED\", \"RUNNER\", \"S1\", \"SAM\", \"SMALL\", \"SPANISH\", \"TAN\", \"TYPE\",           \"U\", \"VALENCIA\", \"VIRGINIA\", \"WHITE\")  # Specify the synsets as a list syn <- list(c(\"CHANDRA\", \"AH114\"), c(\"TG1\", \"VIKRAM\"))  GNdupc <- ProbDup(kwic1 = GN1KWIC, kwic2 = GN2KWIC, method = \"c\",                   excep = exep, fuzzy = TRUE, phonetic = TRUE,                   encoding = \"primary\", semantic = TRUE, syn = syn)  GNdupcView <- ViewProbDup(GNdupc, GN1, GN2, \"SourceCountry\", \"SourceCountry\",                          max.count = 30, select = c(\"INDIA\", \"USA\"), order = \"type\",                          main = \"Groundnut Probable Duplicates\")  library(gridExtra)                                                     grid.arrange(GNdupcView$SummaryGrob)                            } # }"},{"path":"https://aravind-j.github.io/PGRdup/reference/print.KWIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Prints summary of KWIC object. — print.KWIC","title":"Prints summary of KWIC object. — print.KWIC","text":"print.KWIC prints console summary object class KWIC including database fields(columns) used, total number keywords number distinct keywords index.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/print.KWIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prints summary of KWIC object. — print.KWIC","text":"","code":"# S3 method for class 'KWIC' print(x, ...)"},{"path":"https://aravind-j.github.io/PGRdup/reference/print.KWIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prints summary of KWIC object. — print.KWIC","text":"x object class KWIC. ... Unused","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/print.ProbDup.html","id":null,"dir":"Reference","previous_headings":"","what":"Prints summary of ProbDup object. — print.ProbDup","title":"Prints summary of ProbDup object. — print.ProbDup","text":"print.ProbDup prints console summary object class ProbDup including method used (\"\", \"b\" \"c\"), database fields(columns) considered, number probable duplicate sets kind along corresponding number records.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/print.ProbDup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prints summary of ProbDup object. — print.ProbDup","text":"","code":"# S3 method for class 'ProbDup' print(x, ...)"},{"path":"https://aravind-j.github.io/PGRdup/reference/print.ProbDup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prints summary of ProbDup object. — print.ProbDup","text":"x object class ProbDup. ... Unused","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/read.genesys.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","title":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","text":"read.genesys reads PGR data Darwin Core - germplasm zip archive downloaded genesys database creates flat file data.frame .","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/read.genesys.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","text":"","code":"read.genesys(zip.genesys, scrub.names.space = TRUE, readme = TRUE)"},{"path":"https://aravind-j.github.io/PGRdup/reference/read.genesys.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","text":"zip.genesys character vector giving file path downloaded zip file Genesys. scrub.names.space logical. TRUE, space characters removed name field names extension (see Details). readme logical. TRUE, genesys zip file readme printed console.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/read.genesys.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","text":"data.frame flat file form genesys data.","code":""},{"path":"https://aravind-j.github.io/PGRdup/reference/read.genesys.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","text":"function helps import R environment, PGR data downloaded genesys database https://www.genesys-pgr.org/ Darwin Core - germplasm (DwC-germplasm) zip archive. different csv files archive merged flat file single data.frame. space characters can removed fields corresponding accession names acceNumb, collNumb, ACCENAME, COLLNUMB, DONORNUMB OTHERNUMB using argument scrub.names.space facilitate creation KWIC index KWIC function subsequent matching operations identify probable duplicates ProbDup function. argument readme can used print readme file archive console, required.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/reference/read.genesys.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert 'Darwin Core - Germplasm' zip archive to a flat file — read.genesys","text":"","code":"if (FALSE) { # \\dontrun{ # Import the DwC-Germplasm zip archive \"genesys-accessions-filtered.zip\" PGRgenesys <- read.genesys(\"genesys-accessions-filtered.zip\",                            scrub.names.space = TRUE, readme = TRUE) } # }"},{"path":[]},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0240","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.4.0","title":"PGRdup 0.2.4.0","text":"CRAN release: 2025-12-14","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-4-0","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.4.0","text":"Fixed checkRd notes.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0239","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.9","title":"PGRdup 0.2.3.9","text":"CRAN release: 2023-08-31","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3-9","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3.9","text":"Fixed CITATION.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0238","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.8","title":"PGRdup 0.2.3.8","text":"CRAN release: 2023-05-23","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3-8","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3.8","text":"Fixed non-ascii content replaced latin1 encoding utf-8.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0237","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.7","title":"PGRdup 0.2.3.7","text":"CRAN release: 2021-02-17","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"updated-functions-0-2-3-7","dir":"Changelog","previous_headings":"","what":"UPDATED FUNCTIONS:","title":"PGRdup 0.2.3.7","text":"ValidatePrimKey - Fixed xtfrm data.frame.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"vignette-0-2-3-7","dir":"Changelog","previous_headings":"","what":"VIGNETTE:","title":"PGRdup 0.2.3.7","text":"Removed cairo dependency.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0236","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.6","title":"PGRdup 0.2.3.6","text":"CRAN release: 2020-07-27","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"vignette-0-2-3-6","dir":"Changelog","previous_headings":"","what":"VIGNETTE:","title":"PGRdup 0.2.3.6","text":"Reverted using system certificates instead RCurl ones fetching displaying version history suggested Prof. Brian Ripley (ripley@stats.ox.ac.uk).","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0235","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.5","title":"PGRdup 0.2.3.5","text":"CRAN release: 2020-02-10","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3-5","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3.5","text":"Removed reference RecordLinkagepackage removed CRAN.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0234","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.4","title":"PGRdup 0.2.3.4","text":"CRAN release: 2019-09-19","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"updated-functions-0-2-3-4","dir":"Changelog","previous_headings":"","what":"UPDATED FUNCTIONS:","title":"PGRdup 0.2.3.4","text":"MergeKW - Updated regular expressions PCRE2 compliant. read.genesys - Updated reading doi field. DoubleMetaphone - Fixed issue underlying C code ‘strncpy’. Changed specified bound depending length source argument destination argument.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0233","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.3","title":"PGRdup 0.2.3.3","text":"CRAN release: 2018-01-13","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3-3","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3.3","text":"Use packages Suggests (eg. microbenchmark) made conditional avoid problems available OS.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0232","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.2","title":"PGRdup 0.2.3.2","text":"CRAN release: 2017-08-04","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"updated-functions-0-2-3-2","dir":"Changelog","previous_headings":"","what":"UPDATED FUNCTIONS:","title":"PGRdup 0.2.3.2","text":"DoubleMetaphone - Fixed memory leak issues underlying C code.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3-2","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3.2","text":"Minor corrections vignette. Added welcome message. Added version history vignette Replaced 1:nrow() 1:length() usage function seq_len(nrow()) seq_len(length()) respectively. Added package github. Added package documentation website (https://aravind-j.github.io/PGRdup/) github page pkgdown. Added copyright Authors@R along original contributors underlying C code DoubleMetaphone.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0231","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3.1","title":"PGRdup 0.2.3.1","text":"CRAN release: 2017-03-15","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3-1","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3.1","text":"Registered native routines C code DoubleMetaphone.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-023","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.3","title":"PGRdup 0.2.3","text":"CRAN release: 2017-02-01","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"updated-functions-0-2-3","dir":"Changelog","previous_headings":"","what":"UPDATED FUNCTIONS:","title":"PGRdup 0.2.3","text":"KWCounts - Fixed error case large number exceptions fixed bug regarding non-exact removal keyword exceptions. ProbDup - Changed code column vector specifying columns =FALSE argument new preferred syntax data.table. ViewProbDup - Fixed error ‘formal argument “axis.ticks.y” matched multiple actual arguments’. ViewProbDup - Fixed bug case factor names select argument present factor.db1 /factor.db2, function stops. Now gives warning stops none factor names select present factor.db*.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-3","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.3","text":"Added rmarkdown Suggests field DESCRIPTION, prompted Jan Górecki.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-0221","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.2.1","title":"PGRdup 0.2.2.1","text":"CRAN release: 2016-03-09","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-2-1","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.2.1","text":"Fixed memory access error src/fdouble_metaphone.c (Thanks Prof. Brian Ripley)","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-022","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.2","title":"PGRdup 0.2.2","text":"CRAN release: 2016-03-05","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"new-functions-0-2-2","dir":"Changelog","previous_headings":"","what":"NEW FUNCTIONS:","title":"PGRdup 0.2.2","text":"read.genesys - Convert ‘Darwin Core - Germplasm’ zip archive flat file. ViewProbDup - Visualize probable duplicate sets retrieved ProbDup object.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"updated-functions-0-2-2","dir":"Changelog","previous_headings":"","what":"UPDATED FUNCTIONS:","title":"PGRdup 0.2.2","text":"ReconstructProbDup - Fixed bug regarding failure retrieve db2 fields method “c” used. ProbDup - Updated code bugfix stringdist package (stringdistmatrix: output transposed length()==1).","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-2","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.2","text":"Changed contact email addresses four authors (including maintainer) DESCRIPTION. Updated vignette README.md details new functions.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-021","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.1","title":"PGRdup 0.2.1","text":"CRAN release: 2015-07-23","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"new-functions-0-2-1","dir":"Changelog","previous_headings":"","what":"NEW FUNCTIONS:","title":"PGRdup 0.2.1","text":"SplitProbDup - Split object class ProbDup. MergeProbDup - Merges two objects class ProbDup. KWCounts - Generates keyword counts database fields. print.KWIC - Prints summary object class KWIC console. print.ProbDup - Prints summary object class ProbDup console.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"updated-functions-0-2-1","dir":"Changelog","previous_headings":"","what":"UPDATED FUNCTIONS:","title":"PGRdup 0.2.1","text":"ProbDup - Modified phonetic matching better handling strings digits. ProbDup - Fixed throwing error duplicate sets retrieved. ProbDup - Fixed issue regarding memory error large number exceptions . ProbDup - converted code use data.table package greater efficiency speed. ProbDup - Fixed bug regarding inconsistent matching method “b” used. ProbDup - Reduced dimensions string matching matrices produced greater efficiency speed. MergeKW - Modified better handling regex special characters. ReconstructProbDup - Modified ignore sets counts less 2 reconstruction.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"other-notes-0-2-1","dir":"Changelog","previous_headings":"","what":"OTHER NOTES:","title":"PGRdup 0.2.1","text":"Edited README.md formatting. Added diagram, microbenchmark wordcloud (required vignette) suggests field DESCRIPTION. Added imports functions methods, stats utils R CMD check ---cran now checks code usage (via codetools) base package attached. Dropped abbreviation PGR title DESCRIPTION mentioned description text.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"vignette-0-2-1","dir":"Changelog","previous_headings":"","what":"VIGNETTE:","title":"PGRdup 0.2.1","text":"Added vignette “Introduction PGRdup package”.","code":""},{"path":"https://aravind-j.github.io/PGRdup/news/index.html","id":"pgrdup-020","dir":"Changelog","previous_headings":"","what":"PGRdup 0.2.0","title":"PGRdup 0.2.0","text":"First release","code":""}]
